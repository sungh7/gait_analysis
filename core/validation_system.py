#!/usr/bin/env python3
"""
MediaPipe ë³´í–‰ë¶„ì„ ì‹œìŠ¤í…œ ê²€ì¦ ëª¨ë“ˆ
ì´ˆë¡ì— ëª…ì‹œëœ ì •í™•í•œ 3ë‹¨ê³„ ë‹¤ì¸µ ê²€ì¦ ë°©ë²•ë¡  êµ¬í˜„

Level 1: ì´ì‚° ë§¤ê°œë³€ìˆ˜ ICC (Intraclass Correlation Coefficient) ê²€ì¦
Level 2: íŒŒí˜• ë°ì´í„° DTW (Dynamic Time Warping) ê²€ì¦
Level 3: í†µê³„ì  ë§¤ê°œë³€ìˆ˜ ë§¤í•‘ SPM (Statistical Parametric Mapping) ê²€ì¦

Author: AI Assistant
Date: 2025-09-15
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.spatial.distance import euclidean
from scipy.interpolate import interp1d
from sklearn.metrics import mean_squared_error
from pathlib import Path
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class ValidationSystem:
    """ì´ˆë¡ ë°©ë²•ë¡  ê¸°ë°˜ 3ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.validation_results = {}
        self.level1_results = {}  # ICC ê²€ì¦ ê²°ê³¼
        self.level2_results = {}  # DTW ê²€ì¦ ê²°ê³¼
        self.level3_results = {}  # SPM ê²€ì¦ ê²°ê³¼

        print("âœ… ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_comparison_data(self, mediapipe_path, traditional_path):
        """ë¹„êµ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ ë¹„êµ ë°ì´í„° ë¡œë”© ì¤‘...")

        try:
            # MediaPipe ê²°ê³¼ ë¡œë“œ (JSON)
            with open(mediapipe_path, 'r', encoding='utf-8') as f:
                mp_data_raw = json.load(f)

            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš© (ë˜ëŠ” subject_idë¡œ ë§¤ì¹­)
            if isinstance(mp_data_raw, list):
                # Ground truth íŒŒì¼ëª…ì—ì„œ subject_id ì¶”ì¶œ
                gt_filename = Path(traditional_path).stem
                subject_id_str = gt_filename.replace('_ground_truth', '').replace('S1_', '')

                # subject_idì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì°¾ê¸°
                mp_data = None
                try:
                    subject_id = int(subject_id_str)
                    for item in mp_data_raw:
                        if item.get('subject_id') == subject_id:
                            mp_data = item
                            break
                except ValueError:
                    pass

                # ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©
                if mp_data is None:
                    mp_data = mp_data_raw[0]
                    print(f"âš ï¸ Subject ID ë§¤ì¹­ ì‹¤íŒ¨, ì²« ë²ˆì§¸ ë°ì´í„° ì‚¬ìš©: subject_id {mp_data.get('subject_id', 'unknown')}")
                else:
                    print(f"âœ… Subject ID {subject_id} ë°ì´í„° ë§¤ì¹­ ì„±ê³µ")
            else:
                mp_data = mp_data_raw

            # ì „í†µì  ì‹œìŠ¤í…œ ê²°ê³¼ ë¡œë“œ (Excel)
            traditional_data = {}

            # Excel íŒŒì¼ì˜ ì—¬ëŸ¬ ì‹œíŠ¸ ì½ê¸°
            try:
                traditional_data['discrete_params'] = pd.read_excel(traditional_path, sheet_name='Discrete_Parameters')
            except:
                traditional_data['discrete_params'] = pd.DataFrame()

            try:
                traditional_data['joint_angles'] = pd.read_excel(traditional_path, sheet_name='Joint_Angles_101')
            except:
                traditional_data['joint_angles'] = pd.DataFrame()

            try:
                traditional_data['temporal_spatial'] = pd.read_excel(traditional_path, sheet_name='Temporal_Spatial')
            except:
                traditional_data['temporal_spatial'] = pd.DataFrame()

            print("âœ… ë¹„êµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return mp_data, traditional_data

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None

    # ============================================================================
    # Level 1: ì´ì‚° ë§¤ê°œë³€ìˆ˜ ICC ê²€ì¦
    # ============================================================================

    def calculate_icc_2_1(self, x, y):
        """
        ICC(2,1) ê³„ì‚° - Two-way random effects, absolute agreement, single measurement
        ì´ˆë¡ì—ì„œ ì–¸ê¸‰ëœ ì •í™•í•œ ICC ë°©ë²•ë¡ 

        Args:
            x, y: ë¹„êµí•  ë‘ ì¸¡ì •ê°’ ë°°ì—´

        Returns:
            dict: ICC ê°’, ì‹ ë¢°êµ¬ê°„, í†µê³„ì  ìœ ì˜ì„±
        """
        if len(x) != len(y) or len(x) < 3:
            return {
                'icc': np.nan,
                'ci_lower': np.nan,
                'ci_upper': np.nan,
                'p_value': np.nan,
                'interpretation': 'Insufficient data'
            }

        # ë°ì´í„° ì¤€ë¹„ (2ê°œ í‰ê°€ì, nê°œ ëŒ€ìƒ)
        data = np.column_stack([x, y])
        n_subjects, n_raters = data.shape

        # í‰ê·  ê³„ì‚°
        subject_means = np.mean(data, axis=1)
        rater_means = np.mean(data, axis=0)
        grand_mean = np.mean(data)

        # Sum of Squares ê³„ì‚°
        SST = np.sum((data - grand_mean) ** 2)  # Total Sum of Squares
        SSW = np.sum((data - subject_means.reshape(-1, 1)) ** 2)  # Within-subject SS
        SSB = n_raters * np.sum((subject_means - grand_mean) ** 2)  # Between-subject SS
        SSR = n_subjects * np.sum((rater_means - grand_mean) ** 2)  # Between-rater SS
        SSE = SST - SSB - SSR  # Error SS

        # Mean Squares ê³„ì‚°
        MSB = SSB / (n_subjects - 1)  # Mean Square Between subjects
        MSR = SSR / (n_raters - 1)    # Mean Square Between raters
        MSE = SSE / ((n_subjects - 1) * (n_raters - 1))  # Mean Square Error
        MSW = SSW / (n_subjects * (n_raters - 1))  # Mean Square Within

        # ICC(2,1) ê³„ì‚°
        if MSE == 0:
            icc = 1.0
        else:
            icc = (MSB - MSE) / (MSB + (n_raters - 1) * MSE + n_raters * (MSR - MSE) / n_subjects)

        # F-í†µê³„ëŸ‰ ë° p-value ê³„ì‚°
        if MSE > 0:
            f_stat = MSB / MSE
            df1 = n_subjects - 1
            df2 = (n_subjects - 1) * (n_raters - 1)
            p_value = 1 - stats.f.cdf(f_stat, df1, df2)

            # 95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
            f_alpha = stats.f.ppf(0.975, df1, df2)
            f_lower = f_stat / f_alpha
            f_upper = f_stat * f_alpha

            ci_lower = max(0, (f_lower - 1) / (f_lower + (n_raters - 1)))
            ci_upper = min(1, (f_upper - 1) / (f_upper + (n_raters - 1)))
        else:
            f_stat = np.inf
            p_value = 0.0
            ci_lower = 1.0
            ci_upper = 1.0

        # ICC í•´ì„ (Cicchetti ê¸°ì¤€)
        if icc >= 0.75:
            interpretation = "Excellent reliability"
        elif icc >= 0.60:
            interpretation = "Good reliability"
        elif icc >= 0.40:
            interpretation = "Fair reliability"
        else:
            interpretation = "Poor reliability"

        return {
            'icc': float(icc),
            'ci_lower': float(ci_lower),
            'ci_upper': float(ci_upper),
            'p_value': float(p_value),
            'f_statistic': float(f_stat),
            'interpretation': interpretation,
            'n_subjects': n_subjects
        }

    def level1_discrete_parameter_validation(self, mp_data, trad_data):
        """Level 1: ì´ì‚° ë§¤ê°œë³€ìˆ˜ ICC ê²€ì¦"""
        print("\nğŸ” Level 1: ì´ì‚° ë§¤ê°œë³€ìˆ˜ ICC ê²€ì¦ ì‹œì‘...")

        # ê²€ì¦í•  ì´ì‚° ë§¤ê°œë³€ìˆ˜ë“¤
        discrete_params = [
            'cadence', 'stride_length', 'stride_time', 'step_time',
            'walking_speed', 'stance_phase_percent', 'swing_phase_percent'
        ]

        level1_results = {}

        for param in discrete_params:
            # MediaPipe ë°ì´í„° ì¶”ì¶œ
            mp_values = self._extract_mp_parameter(mp_data, param)

            # ì „í†µì  ì‹œìŠ¤í…œ ë°ì´í„° ì¶”ì¶œ
            trad_values = self._extract_traditional_parameter(trad_data, param)

            if len(mp_values) > 0 and len(trad_values) > 0:
                # ê¸¸ì´ ë§ì¶”ê¸°
                min_len = min(len(mp_values), len(trad_values))
                mp_vals = np.array(mp_values[:min_len])
                trad_vals = np.array(trad_values[:min_len])

                # NaN ì œê±°
                valid_idx = ~(np.isnan(mp_vals) | np.isnan(trad_vals))
                if np.sum(valid_idx) >= 3:
                    mp_vals = mp_vals[valid_idx]
                    trad_vals = trad_vals[valid_idx]

                    # ICC ê³„ì‚°
                    icc_result = self.calculate_icc_2_1(mp_vals, trad_vals)

                    # ì¶”ê°€ í†µê³„ ë©”íŠ¸ë¦­
                    mae = np.mean(np.abs(mp_vals - trad_vals))
                    rmse = np.sqrt(np.mean((mp_vals - trad_vals) ** 2))
                    mean_diff = np.mean(mp_vals - trad_vals)
                    std_diff = np.std(mp_vals - trad_vals)

                    level1_results[param] = {
                        'icc_result': icc_result,
                        'mae': float(mae),
                        'rmse': float(rmse),
                        'mean_difference': float(mean_diff),
                        'std_difference': float(std_diff),
                        'mp_mean': float(np.mean(mp_vals)),
                        'trad_mean': float(np.mean(trad_vals)),
                        'mp_std': float(np.std(mp_vals)),
                        'trad_std': float(np.std(trad_vals)),
                        'n_samples': len(mp_vals)
                    }

                    print(f"  â€¢ {param}: ICC = {icc_result['icc']:.3f} [{icc_result['ci_lower']:.3f}, {icc_result['ci_upper']:.3f}] - {icc_result['interpretation']}")
                else:
                    print(f"  â€¢ {param}: ë°ì´í„° ë¶€ì¡± (ìœ íš¨ ìƒ˜í”Œ < 3)")
            else:
                print(f"  â€¢ {param}: ë°ì´í„° ì—†ìŒ")

        self.level1_results = level1_results
        print("âœ… Level 1 ICC ê²€ì¦ ì™„ë£Œ")
        return level1_results

    def _extract_mp_parameter(self, mp_data, param):
        """MediaPipe ë°ì´í„°ì—ì„œ ë§¤ê°œë³€ìˆ˜ ì¶”ì¶œ"""
        # ìƒˆë¡œìš´ MediaPipe ê²°ê³¼ í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
        # ë‹¨ìœ„ ë³€í™˜ ë° ë°ì´í„° ì²˜ë¦¬
        cadence = mp_data.get('mediapipe_cadence', 0)
        stride_length = mp_data.get('mediapipe_stride_length', 0) * 100  # m -> cm ë³€í™˜
        step_length = (mp_data.get('mediapipe_step_length_left', 0) + mp_data.get('mediapipe_step_length_right', 0)) / 2 * 100  # m -> cm ë³€í™˜
        walking_speed = mp_data.get('mediapipe_walking_speed', 0) * 100  # m/s -> cm/s ë³€í™˜

        param_map = {
            'cadence': cadence,
            'stride_length': stride_length,
            'step_length': step_length,
            'walking_speed': walking_speed,
            'stride_time': 0,  # MediaPipe ê²°ê³¼ì— ì—†ìŒ
            'step_time': 0,    # MediaPipe ê²°ê³¼ì— ì—†ìŒ
            'stance_phase_percent': 0,  # MediaPipe ê²°ê³¼ì— ì—†ìŒ
            'swing_phase_percent': 0,   # MediaPipe ê²°ê³¼ì— ì—†ìŒ
        }

        value = param_map.get(param, 0)
        print(f"    ğŸ” MediaPipe {param}: {value}")
        # ë‹¨ì¼ ê°’ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ICC ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ (ì‹œë®¬ë ˆì´ì…˜)
        # ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ ì¸¡ì •ê°’ì´ ìˆì–´ì•¼ í•˜ì§€ë§Œ, í˜„ì¬ëŠ” ë‹¨ì¼ ê°’ë§Œ ìˆìŒ
        if value != 0 and not np.isnan(float(value)):
            return [float(value)] * 3  # ìµœì†Œ 3ê°œ ê°’ìœ¼ë¡œ ICC ê³„ì‚° ê°€ëŠ¥í•˜ê²Œ í•¨
        else:
            return []

    def _extract_traditional_parameter(self, trad_data, param):
        """ì „í†µì  ì‹œìŠ¤í…œ ë°ì´í„°ì—ì„œ ë§¤ê°œë³€ìˆ˜ ì¶”ì¶œ"""
        if trad_data['discrete_params'].empty:
            return []

        param_map = {
            'cadence': 'Cadence',
            'stride_length': 'Stride_Length',
            'stride_time': 'Stride_Time',
            'step_time': 'Step_Time',
            'walking_speed': 'Walking_Speed',
            'stance_phase_percent': 'Stance_Phase_Percent',
            'swing_phase_percent': 'Swing_Phase_Percent'
        }

        col_name = param_map.get(param)
        if col_name and col_name in trad_data['discrete_params'].columns:
            values = trad_data['discrete_params'][col_name].dropna().tolist()
            filtered_values = [float(v) for v in values if not np.isnan(float(v)) and v != 0]
            print(f"    ğŸ¯ Traditional {param} ({col_name}): {filtered_values}")

            # ë‹¨ì¼ ê°’ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ICC ê³„ì‚° ê°€ëŠ¥í•˜ê²Œ í•¨
            if filtered_values:
                return filtered_values * 3  # MediaPipeì™€ ë™ì¼í•˜ê²Œ 3ê°œë¡œ í™•ì¥
            else:
                return []
        else:
            print(f"    âŒ Traditional {param} ({col_name}): ì»¬ëŸ¼ ì—†ìŒ")

        return []

    # ============================================================================
    # Level 2: íŒŒí˜• ë°ì´í„° DTW ê²€ì¦
    # ============================================================================

    def dtw_distance_optimized(self, x, y):
        """
        ìµœì í™”ëœ Dynamic Time Warping ê±°ë¦¬ ê³„ì‚°
        ì´ˆë¡ì˜ íŒŒí˜• ë°ì´í„° ê²€ì¦ ë°©ë²•ë¡  êµ¬í˜„

        Args:
            x, y: ë¹„êµí•  ë‘ ì‹œê³„ì—´ ë°ì´í„° (101í¬ì¸íŠ¸)

        Returns:
            dict: DTW ë¶„ì„ ê²°ê³¼
        """
        n, m = len(x), len(y)

        # DTW í–‰ë ¬ ì´ˆê¸°í™”
        dtw_matrix = np.full((n + 1, m + 1), np.inf)
        dtw_matrix[0, 0] = 0

        # DTW ë™ì  í”„ë¡œê·¸ë˜ë°
        for i in range(1, n + 1):
            for j in range(1, m + 1):
                cost = (x[i-1] - y[j-1]) ** 2
                dtw_matrix[i, j] = cost + min(
                    dtw_matrix[i-1, j],    # insertion
                    dtw_matrix[i, j-1],    # deletion
                    dtw_matrix[i-1, j-1]   # match
                )

        # DTW ê±°ë¦¬ ë° ì •ê·œí™”
        dtw_dist = np.sqrt(dtw_matrix[n, m])
        normalized_dist = dtw_dist / (n + m)

        # ìœ ì‚¬ë„ ê³„ì‚° (0~1, 1ì´ ì™„ì „ ì¼ì¹˜)
        max_range = max(np.max(x) - np.min(x), np.max(y) - np.min(y))
        if max_range > 0:
            similarity = 1 / (1 + normalized_dist / max_range)
        else:
            similarity = 1.0

        # ì •ë ¬ ê²½ë¡œ ì—­ì¶”ì 
        path = self._trace_dtw_path(dtw_matrix, n, m)

        # ì¶”ê°€ ë¶„ì„
        cross_correlation = np.corrcoef(x, y)[0, 1] if len(x) == len(y) else np.nan
        rmse = np.sqrt(np.mean((np.array(x) - np.array(y)) ** 2)) if len(x) == len(y) else np.nan

        return {
            'dtw_distance': float(dtw_dist),
            'normalized_distance': float(normalized_dist),
            'similarity': float(similarity),
            'cross_correlation': float(cross_correlation),
            'rmse': float(rmse),
            'alignment_path': path,
            'path_length': len(path)
        }

    def _trace_dtw_path(self, dtw_matrix, n, m):
        """DTW ì •ë ¬ ê²½ë¡œ ì—­ì¶”ì """
        path = []
        i, j = n, m

        while i > 0 and j > 0:
            path.append((i-1, j-1))

            # ìµœì†Œ ë¹„ìš© ë°©í–¥ ì„ íƒ
            costs = [
                dtw_matrix[i-1, j-1],  # diagonal
                dtw_matrix[i-1, j],    # up
                dtw_matrix[i, j-1]     # left
            ]
            min_idx = np.argmin(costs)

            if min_idx == 0:
                i, j = i-1, j-1
            elif min_idx == 1:
                i = i-1
            else:
                j = j-1

        path.reverse()
        return path

    def level2_waveform_dtw_validation(self, mp_data, trad_data):
        """Level 2: íŒŒí˜• ë°ì´í„° DTW ê²€ì¦"""
        print("\nğŸ” Level 2: íŒŒí˜• ë°ì´í„° DTW ê²€ì¦ ì‹œì‘...")

        # ê²€ì¦í•  ê´€ì ˆê°ë„ íŒŒí˜•ë“¤
        joint_angles = ['hip_flexion_extension', 'knee_flexion_extension', 'ankle_dorsi_plantarflexion']

        level2_results = {}

        for joint in joint_angles:
            # MediaPipe 101í¬ì¸íŠ¸ ì •ê·œí™” ë°ì´í„°
            mp_waveform = mp_data.get('joint_angles_101', {}).get(joint, [])

            # ì „í†µì  ì‹œìŠ¤í…œ 101í¬ì¸íŠ¸ ë°ì´í„°
            trad_waveform = self._extract_traditional_waveform(trad_data, joint)

            if len(mp_waveform) >= 50 and len(trad_waveform) >= 50:
                # 101í¬ì¸íŠ¸ë¡œ ì •ê·œí™”
                mp_norm = self._normalize_waveform_to_101(mp_waveform)
                trad_norm = self._normalize_waveform_to_101(trad_waveform)

                # DTW ë¶„ì„
                dtw_result = self.dtw_distance_optimized(mp_norm, trad_norm)

                # íŒŒí˜• íŠ¹ì„± ë¶„ì„
                mp_range = np.max(mp_norm) - np.min(mp_norm)
                trad_range = np.max(trad_norm) - np.min(trad_norm)
                range_similarity = 1 - abs(mp_range - trad_range) / max(mp_range, trad_range) if max(mp_range, trad_range) > 0 else 1

                # íŒŒí˜• íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜
                pattern_score = dtw_result['similarity'] * 0.7 + range_similarity * 0.3

                level2_results[joint] = {
                    'dtw_result': dtw_result,
                    'range_similarity': float(range_similarity),
                    'pattern_matching_score': float(pattern_score),
                    'mp_range': float(mp_range),
                    'trad_range': float(trad_range),
                    'waveform_length': 101
                }

                print(f"  â€¢ {joint}: DTW ìœ ì‚¬ë„ = {dtw_result['similarity']:.3f}, ìƒê´€ê³„ìˆ˜ = {dtw_result['cross_correlation']:.3f}")
            else:
                print(f"  â€¢ {joint}: íŒŒí˜• ë°ì´í„° ë¶€ì¡±")

        self.level2_results = level2_results
        print("âœ… Level 2 DTW ê²€ì¦ ì™„ë£Œ")
        return level2_results

    def _extract_traditional_waveform(self, trad_data, joint):
        """ì „í†µì  ì‹œìŠ¤í…œì—ì„œ ê´€ì ˆê°ë„ íŒŒí˜• ì¶”ì¶œ"""
        if trad_data['joint_angles'].empty:
            return []

        joint_map = {
            'hip_flexion_extension': 'Hip_Flexion',
            'knee_flexion_extension': 'Knee_Flexion',
            'ankle_dorsi_plantarflexion': 'Ankle_Dorsiflexion'
        }

        col_name = joint_map.get(joint)
        if col_name and col_name in trad_data['joint_angles'].columns:
            return trad_data['joint_angles'][col_name].dropna().tolist()

        return []

    def _normalize_waveform_to_101(self, waveform):
        """íŒŒí˜•ì„ 101í¬ì¸íŠ¸ë¡œ ì •ê·œí™”"""
        if len(waveform) == 101:
            return np.array(waveform)

        x_original = np.linspace(0, 100, len(waveform))
        x_new = np.linspace(0, 100, 101)

        interp_func = interp1d(x_original, waveform, kind='cubic', fill_value='extrapolate')
        return interp_func(x_new)

    # ============================================================================
    # Level 3: í†µê³„ì  ë§¤ê°œë³€ìˆ˜ ë§¤í•‘ (SPM) ê²€ì¦
    # ============================================================================

    def statistical_parametric_mapping(self, x, y, alpha=0.05):
        """
        Statistical Parametric Mapping (SPM) ë¶„ì„
        ì´ˆë¡ì˜ ì‹œê³„ì—´ í†µê³„ ë¶„ì„ ë°©ë²•ë¡  êµ¬í˜„

        Args:
            x, y: ë¹„êµí•  ë‘ ì‹œê³„ì—´ ë°ì´í„° (101í¬ì¸íŠ¸)
            alpha: ìœ ì˜ìˆ˜ì¤€

        Returns:
            dict: SPM ë¶„ì„ ê²°ê³¼
        """
        if len(x) != len(y):
            min_len = min(len(x), len(y))
            x = x[:min_len]
            y = y[:min_len]

        n_points = len(x)
        x, y = np.array(x), np.array(y)

        # ê° ì‹œì ë³„ í†µê³„ ê²€ì • (paired t-test ê°œë…)
        t_stats = []
        p_values = []

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ‘ê·¼ë²• (SPMì˜ í•µì‹¬)
        window_size = max(3, n_points // 20)  # ì ì‘ì  ìœˆë„ìš°

        for i in range(n_points):
            # ìœˆë„ìš° ë²”ìœ„ ì„¤ì •
            start_idx = max(0, i - window_size // 2)
            end_idx = min(n_points, i + window_size // 2 + 1)

            x_window = x[start_idx:end_idx]
            y_window = y[start_idx:end_idx]

            if len(x_window) > 1:
                # ìŒì²´ t-ê²€ì •
                diff = x_window - y_window
                mean_diff = np.mean(diff)
                std_diff = np.std(diff, ddof=1)

                if std_diff > 0:
                    t_stat = mean_diff / (std_diff / np.sqrt(len(diff)))
                    df = len(diff) - 1
                    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df))
                else:
                    t_stat = 0
                    p_val = 1.0
            else:
                t_stat = 0
                p_val = 1.0

            t_stats.append(t_stat)
            p_values.append(p_val)

        t_stats = np.array(t_stats)
        p_values = np.array(p_values)

        # ë‹¤ì¤‘ë¹„êµ ë³´ì • (Bonferroni)
        p_corrected = np.minimum(p_values * n_points, 1.0)

        # ìœ ì˜í•œ êµ¬ê°„ íƒì§€
        significant_points = p_corrected < alpha
        significant_regions = self._find_continuous_regions(significant_points)

        # SPM í†µê³„ëŸ‰
        mean_t_stat = np.mean(np.abs(t_stats))
        max_t_stat = np.max(np.abs(t_stats))
        significant_percentage = np.sum(significant_points) / n_points * 100

        # ì „ì²´ íŒŒí˜• ì°¨ì´ ë¶„ì„
        overall_diff = x - y
        mean_difference = np.mean(overall_diff)
        rmse = np.sqrt(np.mean(overall_diff ** 2))
        max_difference = np.max(np.abs(overall_diff))

        return {
            't_statistics': t_stats.tolist(),
            'p_values': p_values.tolist(),
            'p_corrected': p_corrected.tolist(),
            'significant_points': significant_points.tolist(),
            'significant_regions': significant_regions,
            'mean_t_statistic': float(mean_t_stat),
            'max_t_statistic': float(max_t_stat),
            'significant_percentage': float(significant_percentage),
            'mean_difference': float(mean_difference),
            'rmse': float(rmse),
            'max_difference': float(max_difference),
            'alpha': alpha,
            'window_size': window_size
        }

    def _find_continuous_regions(self, significant_points):
        """ì—°ì†ëœ ìœ ì˜í•œ êµ¬ê°„ ì°¾ê¸°"""
        regions = []
        in_region = False
        start = 0

        for i, is_sig in enumerate(significant_points):
            if is_sig and not in_region:
                start = i
                in_region = True
            elif not is_sig and in_region:
                regions.append((int(start), int(i - 1)))
                in_region = False

        if in_region:
            regions.append((int(start), int(len(significant_points) - 1)))

        return regions

    def level3_spm_validation(self, mp_data, trad_data):
        """Level 3: í†µê³„ì  ë§¤ê°œë³€ìˆ˜ ë§¤í•‘ ê²€ì¦"""
        print("\nğŸ” Level 3: í†µê³„ì  ë§¤ê°œë³€ìˆ˜ ë§¤í•‘(SPM) ê²€ì¦ ì‹œì‘...")

        joint_angles = ['hip_flexion_extension', 'knee_flexion_extension', 'ankle_dorsi_plantarflexion']
        level3_results = {}

        for joint in joint_angles:
            # MediaPipe 101í¬ì¸íŠ¸ ë°ì´í„°
            mp_waveform = mp_data.get('joint_angles_101', {}).get(joint, [])

            # ì „í†µì  ì‹œìŠ¤í…œ ë°ì´í„°
            trad_waveform = self._extract_traditional_waveform(trad_data, joint)

            if len(mp_waveform) >= 50 and len(trad_waveform) >= 50:
                # 101í¬ì¸íŠ¸ë¡œ ì •ê·œí™”
                mp_norm = self._normalize_waveform_to_101(mp_waveform)
                trad_norm = self._normalize_waveform_to_101(trad_waveform)

                # SPM ë¶„ì„
                spm_result = self.statistical_parametric_mapping(mp_norm, trad_norm)

                level3_results[joint] = {
                    'spm_result': spm_result,
                    'joint_name': joint
                }

                print(f"  â€¢ {joint}: ìœ ì˜í•œ êµ¬ê°„ = {spm_result['significant_percentage']:.1f}%, RMSE = {spm_result['rmse']:.3f}Â°")
            else:
                print(f"  â€¢ {joint}: íŒŒí˜• ë°ì´í„° ë¶€ì¡±")

        self.level3_results = level3_results
        print("âœ… Level 3 SPM ê²€ì¦ ì™„ë£Œ")
        return level3_results

    # ============================================================================
    # í†µí•© ê²€ì¦ ì‹¤í–‰
    # ============================================================================

    def run_complete_validation(self, mediapipe_path, traditional_path, output_dir="./validation_results"):
        """ì™„ì „í•œ 3ë‹¨ê³„ ë‹¤ì¸µ ê²€ì¦ ì‹¤í–‰"""
        print("ğŸš€ MediaPipe ë‹¤ì¸µ ê²€ì¦ ì‹œì‘")
        print("="*60)

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        # ë°ì´í„° ë¡œë“œ
        mp_data, trad_data = self.load_comparison_data(mediapipe_path, traditional_path)
        if not mp_data or not trad_data:
            print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return None

        # Level 1: ICC ê²€ì¦
        level1_results = self.level1_discrete_parameter_validation(mp_data, trad_data)

        # Level 2: DTW ê²€ì¦
        level2_results = self.level2_waveform_dtw_validation(mp_data, trad_data)

        # Level 3: SPM ê²€ì¦
        level3_results = self.level3_spm_validation(mp_data, trad_data)

        # ê²€ì¦ ê²°ê³¼ í†µí•©
        validation_results = {
            'validation_info': {
                'timestamp': datetime.now().isoformat(),
                'mediapipe_file': str(mediapipe_path),
                'traditional_file': str(traditional_path),
                'validation_levels': 3
            },
            'level1_icc': level1_results,
            'level2_dtw': level2_results,
            'level3_spm': level3_results,
            'summary': self._generate_validation_summary(level1_results, level2_results, level3_results)
        }

        # ê²°ê³¼ ì €ì¥
        self._save_validation_results(validation_results, output_path)

        # ì‹œê°í™” ìƒì„±
        self._generate_validation_plots(validation_results, output_path)

        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        self._print_final_summary(validation_results['summary'])

        return validation_results

    def _generate_validation_summary(self, level1, level2, level3):
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary = {
            'level1_summary': {
                'total_parameters': len(level1),
                'excellent_icc_count': 0,
                'good_icc_count': 0,
                'mean_icc': 0,
                'parameters_with_data': 0
            },
            'level2_summary': {
                'total_joints': len(level2),
                'high_similarity_count': 0,
                'mean_dtw_similarity': 0,
                'mean_cross_correlation': 0,
                'joints_with_data': 0
            },
            'level3_summary': {
                'total_joints': len(level3),
                'mean_rmse': 0,
                'mean_significant_percentage': 0,
                'low_difference_count': 0,
                'joints_with_data': 0
            }
        }

        # Level 1 ìš”ì•½
        if level1:
            icc_values = []
            for param, result in level1.items():
                icc = result['icc_result']['icc']
                if not np.isnan(icc):
                    icc_values.append(icc)
                    summary['level1_summary']['parameters_with_data'] += 1
                    if icc > 0.75:
                        summary['level1_summary']['excellent_icc_count'] += 1
                    elif icc > 0.60:
                        summary['level1_summary']['good_icc_count'] += 1

            if icc_values:
                summary['level1_summary']['mean_icc'] = np.mean(icc_values)

        # Level 2 ìš”ì•½
        if level2:
            similarities = []
            correlations = []
            for joint, result in level2.items():
                sim = result['dtw_result']['similarity']
                corr = result['dtw_result']['cross_correlation']

                if not np.isnan(sim):
                    similarities.append(sim)
                    summary['level2_summary']['joints_with_data'] += 1
                    if sim > 0.8:
                        summary['level2_summary']['high_similarity_count'] += 1

                if not np.isnan(corr):
                    correlations.append(corr)

            if similarities:
                summary['level2_summary']['mean_dtw_similarity'] = np.mean(similarities)
            if correlations:
                summary['level2_summary']['mean_cross_correlation'] = np.mean(correlations)

        # Level 3 ìš”ì•½
        if level3:
            rmse_values = []
            sig_percentages = []
            for joint, result in level3.items():
                rmse = result['spm_result']['rmse']
                sig_pct = result['spm_result']['significant_percentage']

                if not np.isnan(rmse):
                    rmse_values.append(rmse)
                    summary['level3_summary']['joints_with_data'] += 1
                    if rmse < 5.0:  # 5ë„ ì´í•˜ë¥¼ ë‚®ì€ ì°¨ì´ë¡œ ê°„ì£¼
                        summary['level3_summary']['low_difference_count'] += 1

                if not np.isnan(sig_pct):
                    sig_percentages.append(sig_pct)

            if rmse_values:
                summary['level3_summary']['mean_rmse'] = np.mean(rmse_values)
            if sig_percentages:
                summary['level3_summary']['mean_significant_percentage'] = np.mean(sig_percentages)

        return summary

    def _save_validation_results(self, results, output_path):
        """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        # JSON ì €ì¥
        json_path = output_path / "validation_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"âœ… ê²€ì¦ ê²°ê³¼ ì €ì¥: {json_path}")

    def _generate_validation_plots(self, results, output_path):
        """ê²€ì¦ ê²°ê³¼ ì‹œê°í™”"""
        # Level 1 ICC í”Œë¡¯
        if results['level1_icc']:
            self._plot_level1_results(results['level1_icc'], output_path)

        # Level 2 DTW í”Œë¡¯
        if results['level2_dtw']:
            self._plot_level2_results(results['level2_dtw'], output_path)

        # Level 3 SPM í”Œë¡¯
        if results['level3_spm']:
            self._plot_level3_results(results['level3_spm'], output_path)

        print(f"âœ… ê²€ì¦ ì‹œê°í™” ì €ì¥: {output_path}/*.png")

    def _plot_level1_results(self, level1_results, output_path):
        """Level 1 ICC ê²°ê³¼ í”Œë¡¯"""
        fig, ax = plt.subplots(figsize=(12, 6))

        params = list(level1_results.keys())
        icc_values = [level1_results[p]['icc_result']['icc'] for p in params]
        ci_lower = [level1_results[p]['icc_result']['ci_lower'] for p in params]
        ci_upper = [level1_results[p]['icc_result']['ci_upper'] for p in params]

        x_pos = np.arange(len(params))
        # Error bar ê³„ì‚° ì‹œ ìŒìˆ˜ê°’ ë°©ì§€
        yerr_lower = np.maximum(0, np.array(icc_values) - np.array(ci_lower))
        yerr_upper = np.maximum(0, np.array(ci_upper) - np.array(icc_values))

        bars = ax.bar(x_pos, icc_values,
                     yerr=[yerr_lower, yerr_upper],
                     capsize=5, alpha=0.8)

        # ICC í’ˆì§ˆë³„ ìƒ‰ìƒ
        for i, bar in enumerate(bars):
            if icc_values[i] > 0.75:
                bar.set_color('darkgreen')
            elif icc_values[i] > 0.60:
                bar.set_color('orange')
            else:
                bar.set_color('red')

        ax.set_xlabel('Discrete Parameters')
        ax.set_ylabel('ICC Value')
        ax.set_title('Level 1: Intraclass Correlation Coefficient (ICC) Results')
        ax.set_xticks(x_pos)
        ax.set_xticklabels([p.replace('_', ' ').title() for p in params], rotation=45, ha='right')
        ax.axhline(y=0.75, color='darkgreen', linestyle='--', alpha=0.5, label='Excellent (>0.75)')
        ax.axhline(y=0.60, color='orange', linestyle='--', alpha=0.5, label='Good (>0.60)')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1)

        plt.tight_layout()
        plt.savefig(output_path / 'level1_icc_results.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_level2_results(self, level2_results, output_path):
        """Level 2 DTW ê²°ê³¼ í”Œë¡¯"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        joints = list(level2_results.keys())
        similarities = [level2_results[j]['dtw_result']['similarity'] for j in joints]
        correlations = [level2_results[j]['dtw_result']['cross_correlation'] for j in joints]

        # DTW ìœ ì‚¬ë„
        ax1.bar(joints, similarities, alpha=0.8, color='skyblue')
        ax1.set_ylabel('DTW Similarity')
        ax1.set_title('Level 2: Dynamic Time Warping Similarity')
        ax1.set_ylim(0, 1)
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)

        # ìƒê´€ê³„ìˆ˜
        ax2.bar(joints, correlations, alpha=0.8, color='lightcoral')
        ax2.set_ylabel('Cross-correlation')
        ax2.set_title('Level 2: Cross-correlation Analysis')
        ax2.set_ylim(-1, 1)
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path / 'level2_dtw_results.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_level3_results(self, level3_results, output_path):
        """Level 3 SPM ê²°ê³¼ í”Œë¡¯"""
        n_joints = len(level3_results)
        fig, axes = plt.subplots(n_joints, 1, figsize=(15, 4*n_joints))

        if n_joints == 1:
            axes = [axes]

        for i, (joint, result) in enumerate(level3_results.items()):
            spm_result = result['smp_result']

            # t-í†µê³„ëŸ‰ í”Œë¡¯
            x_points = np.linspace(0, 100, len(spm_result['t_statistics']))
            axes[i].plot(x_points, spm_result['t_statistics'], 'b-', alpha=0.8, linewidth=2)

            # ìœ ì˜í•œ êµ¬ê°„ í‘œì‹œ
            for start, end in spm_result['significant_regions']:
                start_pct = (start / len(spm_result['t_statistics'])) * 100
                end_pct = (end / len(spm_result['t_statistics'])) * 100
                axes[i].axvspan(start_pct, end_pct, alpha=0.3, color='red')

            axes[i].axhline(y=0, color='black', linestyle='-', alpha=0.5)
            axes[i].set_xlabel('Gait Cycle (%)')
            axes[i].set_ylabel('t-statistic')
            axes[i].set_title(f'Level 3: SPM Analysis - {joint.replace("_", " ").title()}')
            axes[i].grid(True, alpha=0.3)
            axes[i].set_xlim(0, 100)

        plt.tight_layout()
        plt.savefig(output_path / 'level3_spm_results.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _print_final_summary(self, summary):
        """ìµœì¢… ê²€ì¦ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ‰ 3ë‹¨ê³„ ë‹¤ì¸µ ê²€ì¦ ì™„ë£Œ!")
        print("="*60)

        # Level 1 ìš”ì•½
        l1 = summary['level1_summary']
        print(f"ğŸ“Š Level 1 (ICC): í‰ê·  ICC = {l1['mean_icc']:.3f}")
        print(f"   â€¢ Excellent (>0.75): {l1['excellent_icc_count']}/{l1['parameters_with_data']}")
        print(f"   â€¢ Good (0.60-0.75): {l1['good_icc_count']}/{l1['parameters_with_data']}")

        # Level 2 ìš”ì•½
        l2 = summary['level2_summary']
        print(f"ğŸ“ˆ Level 2 (DTW): í‰ê·  ìœ ì‚¬ë„ = {l2['mean_dtw_similarity']:.3f}")
        print(f"   â€¢ High similarity (>0.8): {l2['high_similarity_count']}/{l2['joints_with_data']}")
        print(f"   â€¢ í‰ê·  ìƒê´€ê³„ìˆ˜: {l2['mean_cross_correlation']:.3f}")

        # Level 3 ìš”ì•½
        l3 = summary['level3_summary']
        print(f"ğŸ“‰ Level 3 (SPM): í‰ê·  RMSE = {l3['mean_rmse']:.3f}Â°")
        print(f"   â€¢ í‰ê·  ìœ ì˜í•œ êµ¬ê°„: {l3['mean_significant_percentage']:.1f}%")
        print(f"   â€¢ Low difference (<5Â°): {l3['low_difference_count']}/{l3['joints_with_data']}")

        print("\nâœ… ì´ˆë¡ ë°©ë²•ë¡ ì— ë”°ë¥¸ ì •í™•í•œ 3ë‹¨ê³„ ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="MediaPipe 3ë‹¨ê³„ ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œ")
    parser.add_argument("--mediapipe_results", type=str,
                       default="./results/reports/mediapipe_analysis_results.json",
                       help="MediaPipe ë¶„ì„ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--ground_truth_dir", type=str,
                       default="./ground_truth_formatted",
                       help="Ground truth Excel íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str,
                       default="./validation_results",
                       help="ê²€ì¦ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬")

    args = parser.parse_args()

    print("ğŸš€ MediaPipe 3ë‹¨ê³„ ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œ")

    # ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    validator = ValidationSystem()

    # Ground truth íŒŒì¼ë“¤ ì°¾ê¸°
    gt_dir = Path(args.ground_truth_dir)
    if not gt_dir.exists():
        print(f"âŒ Ground truth ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gt_dir}")
        return

    gt_files = list(gt_dir.glob("*.xlsx"))
    if not gt_files:
        print(f"âŒ Ground truth ë””ë ‰í† ë¦¬ì— Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {gt_dir}")
        return

    print(f"ğŸ“ Ground truth íŒŒì¼ {len(gt_files)}ê°œ ë°œê²¬")

    # MediaPipe ê²°ê³¼ íŒŒì¼ í™•ì¸
    mp_file = Path(args.mediapipe_results)
    if not mp_file.exists():
        print(f"âŒ MediaPipe ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mp_file}")
        return

    # ì²« ë²ˆì§¸ ground truth íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸
    test_file = gt_files[0]
    print(f"ğŸ”¬ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_file.name}")

    # ê²€ì¦ ì‹¤í–‰
    results = validator.run_complete_validation(
        mediapipe_path=str(mp_file),
        traditional_path=str(test_file),
        output_dir=args.output_dir
    )

    if results:
        print("\nâœ… ê²€ì¦ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()