#!/usr/bin/env python3
"""
GAVD MediaPipe Feature Extractor
Enhanced MediaPipe Gait Analysis System v2.0 - GAVD Integration

GAVD ë°ì´í„°ì…‹ì˜ 510ê°œ ë¹„ë””ì˜¤ í´ë¦½ì—ì„œ MediaPipe íŠ¹ì§• ì¶”ì¶œ

Author: Research Team
Date: 2025-09-22
"""

import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import json
from pathlib import Path
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict
import ast

class GAVDMediaPipeExtractor:
    """GAVD ë¹„ë””ì˜¤ì—ì„œ MediaPipe íŠ¹ì§• ì¶”ì¶œê¸°"""

    def __init__(self, gavd_path="/data/datasets/GAVD", max_workers=4):
        """
        GAVD MediaPipe ì¶”ì¶œê¸° ì´ˆê¸°í™”

        Args:
            gavd_path: GAVD ë°ì´í„°ì…‹ ê²½ë¡œ
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        """
        self.gavd_path = Path(gavd_path)
        self.videos_path = self.gavd_path / "videos_cut_by_view"
        self.data_path = self.gavd_path / "data"
        self.max_workers = max_workers

        # MediaPipe ì´ˆê¸°í™”
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils

        # ê²°ê³¼ ì €ì¥
        self.extracted_features = []
        self.processing_stats = {
            'total_videos': 0,
            'successful': 0,
            'failed': 0,
            'total_frames': 0,
            'processing_time': 0
        }

        # ìŠ¤ë ˆë“œ ì•ˆì „ ë½
        self.lock = threading.Lock()

        print(f"ğŸ¬ GAVD MediaPipe Feature Extractor ì´ˆê¸°í™”")
        print(f"ğŸ“ ë¹„ë””ì˜¤ ê²½ë¡œ: {self.videos_path}")
        print(f"âš¡ ì›Œì»¤ ìˆ˜: {max_workers}")

    def load_clinical_annotations(self):
        """ì„ìƒ ì£¼ì„ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“– ì„ìƒ ì£¼ì„ ë°ì´í„° ë¡œë“œ...")

        annotation_files = list(self.data_path.glob("GAVD_Clinical_Annotations_*.csv"))
        all_annotations = []

        for file_path in annotation_files:
            df = pd.read_csv(file_path)
            all_annotations.append(df)

        combined_annotations = pd.concat(all_annotations, ignore_index=True)
        print(f"âœ… {len(combined_annotations):,}ê°œ ì£¼ì„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

        return combined_annotations

    def get_video_annotation_pairs(self):
        """ë¹„ë””ì˜¤-ì£¼ì„ ìŒ ìƒì„±"""
        print(f"\nğŸ”— ë¹„ë””ì˜¤-ì£¼ì„ ìŒ ìƒì„±...")

        annotations = self.load_clinical_annotations()
        video_files = list(self.videos_path.glob("*.mp4"))

        pairs = []

        for video_file in video_files:
            filename = video_file.stem
            parts = filename.split('_')

            if len(parts) >= 3:
                video_id = parts[0]
                view = '_'.join(parts[1:-1])
                frame_range = parts[-1]

                # í•´ë‹¹ ë¹„ë””ì˜¤ì˜ ì£¼ì„ ì°¾ê¸°
                video_annotations = annotations[
                    (annotations['id'] == video_id) &
                    (annotations['cam_view'] == view)
                ]

                if not video_annotations.empty:
                    gait_pattern = video_annotations['gait_pat'].iloc[0]
                    dataset_type = video_annotations['dataset'].iloc[0]

                    pairs.append({
                        'video_file': str(video_file),
                        'video_id': video_id,
                        'camera_view': view,
                        'frame_range': frame_range,
                        'gait_pattern': gait_pattern,
                        'dataset_type': dataset_type,
                        'annotation_count': len(video_annotations)
                    })

        print(f"âœ… {len(pairs)}ê°œ ë¹„ë””ì˜¤-ì£¼ì„ ìŒ ìƒì„± ì™„ë£Œ")
        return pairs

    def extract_mediapipe_features(self, video_path, max_frames=None):
        """ë‹¨ì¼ ë¹„ë””ì˜¤ì—ì„œ MediaPipe íŠ¹ì§• ì¶”ì¶œ"""
        features = {
            'success': False,
            'frame_count': 0,
            'processing_time': 0,
            'pose_landmarks': [],
            'world_landmarks': [],
            'visibility_scores': [],
            'error_message': None
        }

        start_time = time.time()

        try:
            # ë¹„ë””ì˜¤ ìº¡ì²˜ ì´ˆê¸°í™”
            cap = cv2.VideoCapture(video_path)

            if not cap.isOpened():
                features['error_message'] = "ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return features

            # MediaPipe Pose ì´ˆê¸°í™”
            with self.mp_pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                enable_segmentation=False,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            ) as pose:

                frame_idx = 0

                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break

                    if max_frames and frame_idx >= max_frames:
                        break

                    # BGRì„ RGBë¡œ ë³€í™˜
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                    # MediaPipe ì²˜ë¦¬
                    results = pose.process(rgb_frame)

                    if results.pose_landmarks:
                        # 2D ëœë“œë§ˆí¬ ì¶”ì¶œ
                        landmarks_2d = []
                        visibility_scores = []

                        for landmark in results.pose_landmarks.landmark:
                            landmarks_2d.extend([landmark.x, landmark.y])
                            visibility_scores.append(landmark.visibility)

                        features['pose_landmarks'].append(landmarks_2d)
                        features['visibility_scores'].append(visibility_scores)

                        # 3D ì›”ë“œ ëœë“œë§ˆí¬ ì¶”ì¶œ
                        if results.pose_world_landmarks:
                            landmarks_3d = []
                            for landmark in results.pose_world_landmarks.landmark:
                                landmarks_3d.extend([landmark.x, landmark.y, landmark.z])
                            features['world_landmarks'].append(landmarks_3d)

                    frame_idx += 1

                cap.release()

                features['frame_count'] = frame_idx
                features['processing_time'] = time.time() - start_time
                features['success'] = len(features['pose_landmarks']) > 0

        except Exception as e:
            features['error_message'] = str(e)
            features['processing_time'] = time.time() - start_time

        return features

    def compute_gait_features(self, pose_landmarks, visibility_scores):
        """ë³´í–‰ íŠ¹ì§• ê³„ì‚°"""
        if not pose_landmarks or len(pose_landmarks) < 10:
            return {}

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        landmarks_array = np.array(pose_landmarks)
        visibility_array = np.array(visibility_scores)

        # í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤ (MediaPipe Pose)
        left_hip = 23 * 2  # x,y ì¢Œí‘œì´ë¯€ë¡œ *2
        right_hip = 24 * 2
        left_knee = 25 * 2
        right_knee = 26 * 2
        left_ankle = 27 * 2
        right_ankle = 28 * 2
        left_heel = 29 * 2
        right_heel = 30 * 2
        left_foot_index = 31 * 2
        right_foot_index = 32 * 2

        gait_features = {}

        try:
            # í‰ê·  visibility ì ìˆ˜
            gait_features['avg_visibility'] = np.mean(visibility_array, axis=0).tolist()

            # ê´€ì ˆ ê°ë„ ê³„ì‚° (í”„ë ˆì„ë³„)
            hip_angles = []
            knee_angles = []
            ankle_angles = []

            for frame_landmarks in landmarks_array:
                if len(frame_landmarks) >= 66:  # 33ê°œ ëœë“œë§ˆí¬ * 2 (x,y)
                    # ì™¼ìª½ ë¬´ë¦ ê°ë„ (ëŒ€í‡´-ì •ê°•ì´)
                    left_knee_angle = self.calculate_angle(
                        [frame_landmarks[left_hip], frame_landmarks[left_hip+1]],
                        [frame_landmarks[left_knee], frame_landmarks[left_knee+1]],
                        [frame_landmarks[left_ankle], frame_landmarks[left_ankle+1]]
                    )

                    # ì˜¤ë¥¸ìª½ ë¬´ë¦ ê°ë„
                    right_knee_angle = self.calculate_angle(
                        [frame_landmarks[right_hip], frame_landmarks[right_hip+1]],
                        [frame_landmarks[right_knee], frame_landmarks[right_knee+1]],
                        [frame_landmarks[right_ankle], frame_landmarks[right_ankle+1]]
                    )

                    knee_angles.append([left_knee_angle, right_knee_angle])

            gait_features['knee_angles'] = knee_angles

            # ë°œëª© ë†’ì´ ë³€í™” (ìˆ˜ì§ ì´ë™)
            left_ankle_y = landmarks_array[:, left_ankle+1] if landmarks_array.shape[1] > left_ankle+1 else []
            right_ankle_y = landmarks_array[:, right_ankle+1] if landmarks_array.shape[1] > right_ankle+1 else []

            gait_features['left_ankle_trajectory'] = left_ankle_y.tolist() if len(left_ankle_y) > 0 else []
            gait_features['right_ankle_trajectory'] = right_ankle_y.tolist() if len(right_ankle_y) > 0 else []

            # ë³´í–‰ ì£¼ê¸° ê²€ì¶œ (ë°œëª© ë†’ì´ ê¸°ë°˜ í”¼í¬ ê²€ì¶œ)
            if len(left_ankle_y) > 10:
                from scipy.signal import find_peaks
                peaks_left, _ = find_peaks(-left_ankle_y, distance=10)  # ìŒìˆ˜ë¡œ í•˜ì—¬ ìµœì†Ÿê°’ ì°¾ê¸°
                peaks_right, _ = find_peaks(-right_ankle_y, distance=10)

                gait_features['left_foot_strikes'] = peaks_left.tolist()
                gait_features['right_foot_strikes'] = peaks_right.tolist()
                gait_features['estimated_cadence'] = len(peaks_left) + len(peaks_right)

        except Exception as e:
            gait_features['calculation_error'] = str(e)

        return gait_features

    def calculate_angle(self, point1, point2, point3):
        """ì„¸ ì ìœ¼ë¡œ ê°ë„ ê³„ì‚°"""
        try:
            # ë²¡í„° ê³„ì‚°
            v1 = np.array([point1[0] - point2[0], point1[1] - point2[1]])
            v2 = np.array([point3[0] - point2[0], point3[1] - point2[1]])

            # ì½”ì‚¬ì¸ ê°’ ê³„ì‚°
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            cos_angle = np.clip(cos_angle, -1.0, 1.0)  # ìˆ˜ì¹˜ ì•ˆì •ì„±

            # ê°ë„ ë³€í™˜ (ë¼ë””ì•ˆ -> ë„)
            angle = np.arccos(cos_angle) * 180 / np.pi
            return angle
        except:
            return 0.0

    def process_single_video(self, video_pair):
        """ë‹¨ì¼ ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        video_path = video_pair['video_file']
        video_name = Path(video_path).name

        print(f"ğŸ¬ ì²˜ë¦¬ ì¤‘: {video_name}")

        # MediaPipe íŠ¹ì§• ì¶”ì¶œ
        mp_features = self.extract_mediapipe_features(video_path, max_frames=300)

        # ë³´í–‰ íŠ¹ì§• ê³„ì‚°
        gait_features = {}
        if mp_features['success'] and mp_features['pose_landmarks']:
            gait_features = self.compute_gait_features(
                mp_features['pose_landmarks'],
                mp_features['visibility_scores']
            )

        # ê²°ê³¼ êµ¬ì¡°í™”
        result = {
            'video_info': video_pair,
            'mediapipe_features': {
                'success': mp_features['success'],
                'frame_count': mp_features['frame_count'],
                'processing_time': mp_features['processing_time'],
                'error_message': mp_features.get('error_message'),
                'landmark_count': len(mp_features['pose_landmarks']),
                'world_landmark_count': len(mp_features['world_landmarks'])
            },
            'gait_features': gait_features,
            'extraction_timestamp': datetime.now().isoformat()
        }

        # ìŠ¤ë ˆë“œ ì•ˆì „ í†µê³„ ì—…ë°ì´íŠ¸
        with self.lock:
            self.processing_stats['total_videos'] += 1
            if mp_features['success']:
                self.processing_stats['successful'] += 1
            else:
                self.processing_stats['failed'] += 1
            self.processing_stats['total_frames'] += mp_features['frame_count']
            self.processing_stats['processing_time'] += mp_features['processing_time']

        print(f"âœ… {video_name}: {mp_features['frame_count']}í”„ë ˆì„, "
              f"{mp_features['processing_time']:.1f}ì´ˆ, "
              f"ì„±ê³µ: {mp_features['success']}")

        return result

    def extract_features_batch(self, limit_videos=None):
        """ë°°ì¹˜ íŠ¹ì§• ì¶”ì¶œ"""
        print(f"\nğŸš€ GAVD MediaPipe íŠ¹ì§• ì¶”ì¶œ ì‹œì‘")
        print("=" * 60)

        # ë¹„ë””ì˜¤-ì£¼ì„ ìŒ ê°€ì ¸ì˜¤ê¸°
        video_pairs = self.get_video_annotation_pairs()

        if limit_videos:
            video_pairs = video_pairs[:limit_videos]
            print(f"ğŸ“ ì œí•œ: {limit_videos}ê°œ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬")

        self.processing_stats['total_videos'] = 0  # ë¦¬ì…‹
        start_time = time.time()

        print(f"ğŸ¬ ì´ {len(video_pairs)}ê°œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜ˆì •")
        print(f"âš¡ {self.max_workers}ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬")

        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  ì‘ì—… ì œì¶œ
            future_to_video = {
                executor.submit(self.process_single_video, video_pair): video_pair
                for video_pair in video_pairs
            }

            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_video):
                video_pair = future_to_video[future]
                try:
                    result = future.result()
                    self.extracted_features.append(result)
                except Exception as e:
                    print(f"âŒ {video_pair['video_file']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        total_time = time.time() - start_time

        # ìµœì¢… í†µê³„
        print(f"\nğŸ‰ GAVD MediaPipe íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"   ì´ ë¹„ë””ì˜¤: {len(video_pairs)}")
        print(f"   ì„±ê³µ: {self.processing_stats['successful']}")
        print(f"   ì‹¤íŒ¨: {self.processing_stats['failed']}")
        print(f"   ì„±ê³µë¥ : {self.processing_stats['successful']/len(video_pairs)*100:.1f}%")
        print(f"   ì´ í”„ë ˆì„: {self.processing_stats['total_frames']:,}")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   í‰ê·  ì²˜ë¦¬ ì†ë„: {self.processing_stats['total_frames']/total_time:.1f} FPS")

        return self.extracted_features

    def analyze_extracted_features(self):
        """ì¶”ì¶œëœ íŠ¹ì§• ë¶„ì„"""
        if not self.extracted_features:
            print("âŒ ì¶”ì¶œëœ íŠ¹ì§•ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        print(f"\nğŸ“ˆ ì¶”ì¶œëœ íŠ¹ì§• ë¶„ì„...")

        # ê¸°ë³¸ í†µê³„
        successful_extractions = [f for f in self.extracted_features if f['mediapipe_features']['success']]

        # íŒ¨í„´ë³„ ë¶„ì„
        pattern_stats = defaultdict(list)
        for feature in successful_extractions:
            pattern = feature['video_info']['gait_pattern']
            pattern_stats[pattern].append(feature)

        analysis = {
            'total_videos': len(self.extracted_features),
            'successful_extractions': len(successful_extractions),
            'success_rate': len(successful_extractions) / len(self.extracted_features) * 100,
            'pattern_distribution': {pattern: len(features) for pattern, features in pattern_stats.items()},
            'average_frames_per_video': np.mean([f['mediapipe_features']['frame_count'] for f in successful_extractions]),
            'average_processing_time': np.mean([f['mediapipe_features']['processing_time'] for f in successful_extractions])
        }

        print(f"ğŸ“Š íŠ¹ì§• ì¶”ì¶œ ë¶„ì„ ê²°ê³¼:")
        print(f"   ì´ ë¹„ë””ì˜¤: {analysis['total_videos']}")
        print(f"   ì„±ê³µì  ì¶”ì¶œ: {analysis['successful_extractions']}")
        print(f"   ì„±ê³µë¥ : {analysis['success_rate']:.1f}%")
        print(f"   í‰ê·  í”„ë ˆì„/ë¹„ë””ì˜¤: {analysis['average_frames_per_video']:.1f}")
        print(f"   í‰ê·  ì²˜ë¦¬ì‹œê°„: {analysis['average_processing_time']:.1f}ì´ˆ")

        print(f"\nğŸ¦´ íŒ¨í„´ë³„ ë¶„í¬:")
        for pattern, count in analysis['pattern_distribution'].items():
            percentage = count / len(successful_extractions) * 100
            print(f"   {pattern}: {count}ê°œ ({percentage:.1f}%)")

        return analysis

    def save_extracted_features(self, output_file=None):
        """ì¶”ì¶œëœ íŠ¹ì§• ì €ì¥"""
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"gavd_mediapipe_features_{timestamp}.json"

        # ì „ì²´ ê²°ê³¼ êµ¬ì¡°
        full_results = {
            'extraction_info': {
                'timestamp': datetime.now().isoformat(),
                'total_videos_processed': len(self.extracted_features),
                'processing_stats': self.processing_stats,
                'gavd_dataset_path': str(self.gavd_path)
            },
            'analysis_summary': self.analyze_extracted_features(),
            'extracted_features': self.extracted_features
        }

        # JSON ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, indent=2, ensure_ascii=False)

        file_size = Path(output_file).stat().st_size / (1024*1024)  # MB
        print(f"\nğŸ’¾ íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"   íŒŒì¼ í¬ê¸°: {file_size:.1f} MB")

        return output_file

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¬ GAVD MediaPipe íŠ¹ì§• ì¶”ì¶œ ì‹œì‘")
    print("=" * 60)

    # ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = GAVDMediaPipeExtractor(max_workers=6)

    try:
        # íŠ¹ì§• ì¶”ì¶œ (ì²˜ìŒì—ëŠ” 50ê°œ ë¹„ë””ì˜¤ë¡œ í…ŒìŠ¤íŠ¸)
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 50ê°œ ë¹„ë””ì˜¤ë¡œ ì‹œì‘")
        extracted_features = extractor.extract_features_batch(limit_videos=50)

        # ë¶„ì„
        analysis = extractor.analyze_extracted_features()

        # ì €ì¥
        output_file = extractor.save_extracted_features()

        print(f"\nğŸ‰ GAVD MediaPipe íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_file}")
        print(f"ğŸ¦´ {analysis['successful_extractions']}ê°œ ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì§• ì¶”ì¶œ ì„±ê³µ")

        # ì„±ê³µë¥ ì´ ì¢‹ë‹¤ë©´ ì „ì²´ ì²˜ë¦¬ ì œì•ˆ
        if analysis['success_rate'] > 80:
            print(f"\nâœ¨ ì„±ê³µë¥ ì´ {analysis['success_rate']:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤!")
            print(f"ğŸ’¡ ì „ì²´ {extractor.get_video_annotation_pairs().__len__()}ê°œ ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()