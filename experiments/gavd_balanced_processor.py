#!/usr/bin/env python3
"""
GAVD Balanced Dataset Processor
Enhanced MediaPipe Gait Analysis System v3.0 - ê· í˜•ì¡íŒ ë°ì´í„° ì²˜ë¦¬

ì •ìƒ/ë³‘ì ë³´í–‰ì´ ê· í˜•ì¡íˆê²Œ í¬í•¨ëœ GAVD ë°ì´í„°ì…‹ ì²˜ë¦¬

Author: Research Team
Date: 2025-09-22
"""

import numpy as np
import pandas as pd
import json
from pathlib import Path
from collections import defaultdict
import random
from gavd_optimized_mediapipe_extractor import GAVDOptimizedProcessor, process_single_video_optimized
from gavd_dataset_analyzer import GAVDDatasetAnalyzer
from concurrent.futures import ProcessPoolExecutor, as_completed
import time

class GAVDBalancedProcessor:
    """ê· í˜•ì¡íŒ GAVD ë°ì´í„°ì…‹ ì²˜ë¦¬ê¸°"""

    def __init__(self, max_workers=6):
        """
        ê· í˜•ì¡íŒ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”

        Args:
            max_workers: ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜
        """
        self.max_workers = max_workers
        self.analyzer = GAVDDatasetAnalyzer()
        self.side_view_pairs = []
        self.results = []

        print(f"âš–ï¸  GAVD ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”")
        print(f"ğŸ”„ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ")

    def load_and_balance_dataset(self, target_samples_per_class=15):
        """ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë¡œë“œ ë° ìƒ˜í”Œë§"""
        print(f"\nğŸ“Š ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ êµ¬ì„±...")

        # GAVD ë°ì´í„° ë¡œë“œ
        self.analyzer.load_clinical_annotations()
        all_pairs = self.analyzer.match_videos_with_annotations(side_view_only=True)

        print(f"   ì „ì²´ side view ìŒ: {len(all_pairs)}ê°œ")

        # íŒ¨í„´ë³„ ê·¸ë£¹í™”
        pattern_groups = defaultdict(list)
        for pair in all_pairs:
            pattern = pair['gait_pattern']
            pattern_groups[pattern].append(pair)

        print(f"\nğŸ“ˆ íŒ¨í„´ë³„ ë¶„í¬:")
        for pattern, pairs in pattern_groups.items():
            print(f"   {pattern}: {len(pairs)}ê°œ")

        # ê· í˜•ì¡íŒ ìƒ˜í”Œë§
        balanced_pairs = []

        # ë¨¼ì € ì •ìƒ ë³´í–‰ í™•ë³´
        if 'normal' in pattern_groups:
            normal_pairs = pattern_groups['normal']
            normal_sample_count = min(target_samples_per_class, len(normal_pairs))
            selected_normal = random.sample(normal_pairs, normal_sample_count)
            balanced_pairs.extend(selected_normal)
            print(f"âœ… normal: {len(selected_normal)}ê°œ ì„ íƒ")

        # ë³‘ì  ë³´í–‰ íŒ¨í„´ë“¤ ê· í˜•ìˆê²Œ ìƒ˜í”Œë§
        pathological_patterns = [p for p in pattern_groups.keys() if p != 'normal']

        for pattern in pathological_patterns:
            pairs = pattern_groups[pattern]
            # í´ë˜ìŠ¤ë³„ ìµœì†Œ 3ê°œëŠ” í™•ë³´í•˜ë˜, ëª©í‘œ ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡
            sample_count = min(max(3, target_samples_per_class), len(pairs))
            if len(pairs) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì¸ íŒ¨í„´ë§Œ
                selected_pairs = random.sample(pairs, sample_count)
                balanced_pairs.extend(selected_pairs)
                print(f"âœ… {pattern}: {len(selected_pairs)}ê°œ ì„ íƒ")
            else:
                print(f"âš ï¸  {pattern}: {len(pairs)}ê°œ (ìµœì†Œ 3ê°œ ë¯¸ë§Œìœ¼ë¡œ ì œì™¸)")

        # ëœë¤ ì„ê¸°
        random.shuffle(balanced_pairs)

        self.side_view_pairs = balanced_pairs

        print(f"\nâš–ï¸  ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ:")
        print(f"   ì´ ì„ íƒëœ ìƒ˜í”Œ: {len(self.side_view_pairs)}ê°œ")

        # ìµœì¢… ë¶„í¬ í™•ì¸
        final_distribution = defaultdict(int)
        for pair in self.side_view_pairs:
            final_distribution[pair['gait_pattern']] += 1

        print(f"\nğŸ“Š ìµœì¢… ê· í˜•ì¡íŒ ë¶„í¬:")
        for pattern, count in final_distribution.items():
            print(f"   {pattern}: {count}ê°œ")

        return self.side_view_pairs

    def process_balanced_dataset(self):
        """ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        if not self.side_view_pairs:
            print(f"âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € load_and_balance_dataset()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return []

        print(f"\nğŸš€ ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
        print(f"   ì²˜ë¦¬í•  ë¹„ë””ì˜¤: {len(self.side_view_pairs)}ê°œ")
        print(f"   ì›Œì»¤ ìˆ˜: {self.max_workers}ê°œ")

        start_time = time.time()
        successful = 0
        failed = 0

        # ProcessPoolExecutor ì‚¬ìš©
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  ì‘ì—… ì œì¶œ
            future_to_video = {
                executor.submit(process_single_video_optimized, video): video
                for video in self.side_view_pairs
            }

            # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
            for i, future in enumerate(as_completed(future_to_video), 1):
                try:
                    result = future.result(timeout=60)  # ë¹„ë””ì˜¤ë‹¹ ìµœëŒ€ 1ë¶„
                    self.results.append(result)

                    if result['success']:
                        successful += 1
                        print(f"âœ… [{i}/{len(self.side_view_pairs)}] {result['video_id']} "
                              f"({result['gait_pattern']}) - {result['processing_time']:.1f}s")
                    else:
                        failed += 1
                        print(f"âŒ [{i}/{len(self.side_view_pairs)}] {result['video_id']} - {result['error']}")

                except Exception as e:
                    failed += 1
                    video = future_to_video[future]
                    print(f"âŒ [{i}/{len(self.side_view_pairs)}] {video['video_id']} - ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        total_time = time.time() - start_time

        print(f"\nğŸ“Š ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"   ì„±ê³µ: {successful}ê°œ")
        print(f"   ì‹¤íŒ¨: {failed}ê°œ")
        print(f"   ì„±ê³µë¥ : {successful/(successful+failed)*100:.1f}%")

        # ì„±ê³µí•œ ê²°ê³¼ì˜ íŒ¨í„´ ë¶„í¬ í™•ì¸
        successful_results = [r for r in self.results if r['success']]
        pattern_distribution = defaultdict(int)
        for result in successful_results:
            pattern_distribution[result['gait_pattern']] += 1

        print(f"\nğŸ¯ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ íŒ¨í„´ ë¶„í¬:")
        for pattern, count in pattern_distribution.items():
            print(f"   {pattern}: {count}ê°œ")

        return self.results

    def save_balanced_results(self, output_file=None):
        """ê· í˜•ì¡íŒ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥"""
        if not self.results:
            print(f"âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        timestamp = time.strftime('%Y%m%d_%H%M%S')

        if output_file is None:
            output_file = f"gavd_balanced_results_{timestamp}.json"

        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        successful_results = [r for r in self.results if r['success']]

        results_data = {
            'extraction_info': {
                'timestamp': timestamp,
                'processing_type': 'Balanced Dataset',
                'total_processed': len(self.results),
                'successful': len(successful_results),
                'failed': len(self.results) - len(successful_results),
                'workers': self.max_workers,
                'sampling_strategy': 'balanced_per_class'
            },
            'successful_results': successful_results,
            'processing_stats': {
                'avg_processing_time': np.mean([r.get('processing_time', 0) for r in successful_results]),
                'avg_processing_fps': np.mean([r.get('processing_fps', 0) for r in successful_results])
            },
            'pattern_distribution': {
                pattern: len([r for r in successful_results if r['gait_pattern'] == pattern])
                for pattern in set(r['gait_pattern'] for r in successful_results)
            }
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ê· í˜•ì¡íŒ ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"   ì„±ê³µí•œ ê²°ê³¼: {len(successful_results)}ê°œ")
        print(f"   íŒ¨í„´ ë¶„í¬: {results_data['pattern_distribution']}")

        return output_file

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("âš–ï¸  GAVD ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ì²˜ë¦¬ê¸°")
    print("=" * 50)

    try:
        # ê· í˜•ì¡íŒ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = GAVDBalancedProcessor(max_workers=6)

        # 1. ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë¡œë“œ (íŒ¨í„´ë‹¹ 10ê°œì”©)
        print(f"\nğŸ“Š 1ë‹¨ê³„: ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ êµ¬ì„±")
        balanced_pairs = processor.load_and_balance_dataset(target_samples_per_class=10)

        if not balanced_pairs:
            print(f"âŒ ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ êµ¬ì„± ì‹¤íŒ¨")
            return

        # 2. ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ì²˜ë¦¬
        print(f"\nğŸš€ 2ë‹¨ê³„: ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ì²˜ë¦¬")
        results = processor.process_balanced_dataset()

        # 3. ê²°ê³¼ ì €ì¥
        print(f"\nğŸ’¾ 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
        output_file = processor.save_balanced_results()

        print(f"\nğŸ‰ ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {output_file}")

        # 4. ê°„ë‹¨í•œ í†µê³„
        successful_results = [r for r in results if r['success']]
        if successful_results:
            normal_count = len([r for r in successful_results if r['gait_pattern'] == 'normal'])
            pathological_count = len(successful_results) - normal_count

            print(f"\nğŸ“ˆ ìµœì¢… í†µê³„:")
            print(f"   ì •ìƒ ë³´í–‰: {normal_count}ê°œ")
            print(f"   ë³‘ì  ë³´í–‰: {pathological_count}ê°œ")
            print(f"   ì´ ì„±ê³µ: {len(successful_results)}ê°œ")

            if normal_count > 0 and pathological_count > 0:
                print(f"âœ… ì •ìƒ/ë³‘ì  ë³´í–‰ ë°ì´í„° ê· í˜• í™•ë³´ - ë¶„ë¥˜ í•™ìŠµ ê°€ëŠ¥!")
            else:
                print(f"âš ï¸  ì •ìƒ ë˜ëŠ” ë³‘ì  ë³´í–‰ ë°ì´í„° ë¶€ì¡±")

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •
    random.seed(42)
    np.random.seed(42)

    main()