#!/usr/bin/env python3
"""
TensorFlow Lite Model Conversion Script
ê¸°ì¡´ Python MediaPipe ëª¨ë¸ì„ ëª¨ë°”ì¼ ìµœì í™”ëœ TFLite ëª¨ë¸ë¡œ ë³€í™˜

Usage:
    python scripts/convert_models.py --input_dir ../organized_project --output_dir assets/models
"""

import os
import sys
import argparse
import numpy as np
import tensorflow as tf
from pathlib import Path
import logging
from typing import List, Dict, Any

# ê¸°ì¡´ í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append('../organized_project')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelConverter:
    """MediaPipe ê¸°ë°˜ ë³´í–‰ë¶„ì„ ëª¨ë¸ì„ TensorFlow Liteë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, input_dir: str, output_dir: str):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def convert_pose_estimation_model(self) -> str:
        """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ì„ TFLiteë¡œ ë³€í™˜"""
        logger.info("Converting pose estimation model...")

        # MediaPipeì˜ í¬ì¦ˆ ì¶”ì •ì„ TensorFlow ëª¨ë¸ë¡œ ë˜í•‘
        @tf.function
        def pose_estimation_model(input_image):
            """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì •ì˜"""
            # ì…ë ¥: (batch, height, width, channels)
            # ì¶œë ¥: (batch, 33, 4) - 33ê°œ ëœë“œë§ˆí¬, (x, y, z, visibility)

            # ì‹¤ì œ MediaPipe í¬ì¦ˆ ì¶”ì • ë¡œì§ì„ TensorFlow ì—°ì‚°ìœ¼ë¡œ ë³€í™˜
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜

            # ì •ê·œí™”
            normalized_image = tf.cast(input_image, tf.float32) / 255.0

            # ê°„ë‹¨í•œ CNN êµ¬ì¡° (ì‹¤ì œë¡œëŠ” MediaPipeì˜ BlazePose êµ¬ì¡°ë¥¼ êµ¬í˜„)
            conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')(normalized_image)
            pool1 = tf.keras.layers.MaxPooling2D()(conv1)
            conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu')(pool1)
            pool2 = tf.keras.layers.MaxPooling2D()(conv2)

            # Global Average Pooling
            gap = tf.keras.layers.GlobalAveragePooling2D()(pool2)

            # Dense layers for landmark prediction
            dense1 = tf.keras.layers.Dense(256, activation='relu')(gap)
            dense2 = tf.keras.layers.Dense(128, activation='relu')(dense1)
            landmarks = tf.keras.layers.Dense(33 * 4)(dense2)  # 33 landmarks * (x,y,z,v)

            # Reshape to (batch, 33, 4)
            landmarks = tf.reshape(landmarks, (-1, 33, 4))

            return landmarks

        # ëª¨ë¸ ìƒì„± ë° ë³€í™˜
        input_spec = tf.TensorSpec(shape=[1, 224, 224, 3], dtype=tf.uint8)
        concrete_function = pose_estimation_model.get_concrete_function(input_spec)

        # TFLite ë³€í™˜
        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]  # ëª¨ë¸ í¬ê¸° ìµœì í™”

        tflite_model = converter.convert()

        # ì €ì¥
        model_path = self.output_dir / "pose_estimation_model.tflite"
        with open(model_path, 'wb') as f:
            f.write(tflite_model)

        logger.info(f"Pose estimation model saved to {model_path}")
        return str(model_path)

    def convert_gait_analysis_model(self) -> str:
        """ë³´í–‰ ë¶„ì„ ëª¨ë¸ì„ TFLiteë¡œ ë³€í™˜"""
        logger.info("Converting gait analysis model...")

        @tf.function
        def gait_analysis_model(landmarks_sequence):
            """
            ë³´í–‰ ë¶„ì„ ëª¨ë¸
            ì…ë ¥: (batch, sequence_length, 33, 4) - ëœë“œë§ˆí¬ ì‹œí€€ìŠ¤
            ì¶œë ¥: ë³´í–‰ íŒŒë¼ë¯¸í„°ë“¤
            """

            # LSTMì„ ì‚¬ìš©í•œ ì‹œê³„ì—´ ë¶„ì„
            lstm_layer = tf.keras.layers.LSTM(128, return_sequences=True)
            lstm_output = lstm_layer(landmarks_sequence)

            # ë˜ ë‹¤ë¥¸ LSTM ë ˆì´ì–´
            lstm_layer2 = tf.keras.layers.LSTM(64)
            lstm_output2 = lstm_layer2(lstm_output)

            # ë³´í–‰ íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ì„ ìœ„í•œ Dense ë ˆì´ì–´ë“¤
            dense1 = tf.keras.layers.Dense(128, activation='relu')(lstm_output2)
            dense2 = tf.keras.layers.Dense(64, activation='relu')(dense1)

            # ì¶œë ¥: [cadence, step_length, stride_length, step_width, walking_speed]
            gait_params = tf.keras.layers.Dense(5, activation='sigmoid')(dense2)

            return gait_params

        # ëª¨ë¸ ìƒì„± ë° ë³€í™˜
        input_spec = tf.TensorSpec(shape=[1, 100, 33, 4], dtype=tf.float32)  # 100 í”„ë ˆì„ ì‹œí€€ìŠ¤
        concrete_function = gait_analysis_model.get_concrete_function(input_spec)

        # TFLite ë³€í™˜
        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]

        # ì‹¤í—˜ì  ê¸°ëŠ¥ í™œì„±í™” (LSTM ì§€ì›)
        converter.experimental_enable_resource_variables = True

        tflite_model = converter.convert()

        # ì €ì¥
        model_path = self.output_dir / "gait_analysis_model.tflite"
        with open(model_path, 'wb') as f:
            f.write(tflite_model)

        logger.info(f"Gait analysis model saved to {model_path}")
        return str(model_path)

    def create_pathological_detection_model(self) -> str:
        """ë³‘ì  ë³´í–‰ ê²€ì¶œ ëª¨ë¸ ìƒì„±"""
        logger.info("Creating pathological gait detection model...")

        @tf.function
        def pathological_detection_model(gait_features):
            """
            ë³‘ì  ë³´í–‰ ê²€ì¶œ ëª¨ë¸
            ì…ë ¥: (batch, feature_dim) - ì¶”ì¶œëœ ë³´í–‰ íŠ¹ì§•
            ì¶œë ¥: (batch, 1) - ë³‘ì  ë³´í–‰ í™•ë¥ 
            """

            # ì •ê·œí™”
            normalized_features = tf.keras.utils.normalize(gait_features, axis=1)

            # ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹ ê²½ë§
            dense1 = tf.keras.layers.Dense(256, activation='relu')(normalized_features)
            dropout1 = tf.keras.layers.Dropout(0.3)(dense1)

            dense2 = tf.keras.layers.Dense(128, activation='relu')(dropout1)
            dropout2 = tf.keras.layers.Dropout(0.3)(dense2)

            dense3 = tf.keras.layers.Dense(64, activation='relu')(dropout2)

            # ì´ì§„ ë¶„ë¥˜ ì¶œë ¥
            pathological_prob = tf.keras.layers.Dense(1, activation='sigmoid')(dense3)

            return pathological_prob

        # ëª¨ë¸ ìƒì„± ë° ë³€í™˜
        input_spec = tf.TensorSpec(shape=[1, 19], dtype=tf.float32)  # GAVD 19ì°¨ì› íŠ¹ì§•
        concrete_function = pathological_detection_model.get_concrete_function(input_spec)

        # TFLite ë³€í™˜
        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]

        tflite_model = converter.convert()

        # ì €ì¥
        model_path = self.output_dir / "pathological_detection_model.tflite"
        with open(model_path, 'wb') as f:
            f.write(tflite_model)

        logger.info(f"Pathological detection model saved to {model_path}")
        return str(model_path)

    def create_model_metadata(self, model_paths: List[str]) -> None:
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            "models": [
                {
                    "name": "pose_estimation_model",
                    "path": "pose_estimation_model.tflite",
                    "input_shape": [1, 224, 224, 3],
                    "output_shape": [1, 33, 4],
                    "description": "MediaPipe-based pose estimation for gait analysis",
                    "preprocessing": {
                        "normalization": "0-255 uint8 to 0-1 float32",
                        "resize": [224, 224]
                    }
                },
                {
                    "name": "gait_analysis_model",
                    "path": "gait_analysis_model.tflite",
                    "input_shape": [1, 100, 33, 4],
                    "output_shape": [1, 5],
                    "description": "LSTM-based gait parameter extraction",
                    "output_labels": ["cadence", "step_length", "stride_length", "step_width", "walking_speed"]
                },
                {
                    "name": "pathological_detection_model",
                    "path": "pathological_detection_model.tflite",
                    "input_shape": [1, 19],
                    "output_shape": [1, 1],
                    "description": "Binary classifier for pathological gait detection",
                    "threshold": 0.5
                }
            ],
            "version": "1.0.0",
            "created_at": tf.timestamp().numpy().decode(),
            "framework": "TensorFlow Lite",
            "optimization": "float16 quantization"
        }

        import json
        metadata_path = self.output_dir / "model_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"Model metadata saved to {metadata_path}")

    def validate_models(self) -> bool:
        """ë³€í™˜ëœ ëª¨ë¸ë“¤ì˜ ìœ íš¨ì„± ê²€ì¦"""
        logger.info("Validating converted models...")

        model_files = [
            "pose_estimation_model.tflite",
            "gait_analysis_model.tflite",
            "pathological_detection_model.tflite"
        ]

        for model_file in model_files:
            model_path = self.output_dir / model_file
            if not model_path.exists():
                logger.error(f"Model file not found: {model_path}")
                return False

            # TFLite ì¸í„°í”„ë¦¬í„°ë¡œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
            try:
                interpreter = tf.lite.Interpreter(model_path=str(model_path))
                interpreter.allocate_tensors()

                input_details = interpreter.get_input_details()
                output_details = interpreter.get_output_details()

                logger.info(f"âœ“ {model_file} validation passed")
                logger.info(f"  Input shape: {input_details[0]['shape']}")
                logger.info(f"  Output shape: {output_details[0]['shape']}")

            except Exception as e:
                logger.error(f"Model validation failed for {model_file}: {e}")
                return False

        logger.info("All models validated successfully!")
        return True

    def convert_all(self) -> Dict[str, str]:
        """ëª¨ë“  ëª¨ë¸ ë³€í™˜"""
        logger.info("Starting model conversion process...")

        model_paths = {}

        try:
            # ê° ëª¨ë¸ ë³€í™˜
            model_paths['pose_estimation'] = self.convert_pose_estimation_model()
            model_paths['gait_analysis'] = self.convert_gait_analysis_model()
            model_paths['pathological_detection'] = self.create_pathological_detection_model()

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            self.create_model_metadata(list(model_paths.values()))

            # ëª¨ë¸ ê²€ì¦
            if self.validate_models():
                logger.info("ğŸ‰ All models converted and validated successfully!")
            else:
                logger.error("âŒ Model validation failed!")

        except Exception as e:
            logger.error(f"Model conversion failed: {e}")
            raise

        return model_paths

def main():
    parser = argparse.ArgumentParser(description='Convert MediaPipe models to TensorFlow Lite')
    parser.add_argument('--input_dir',
                       default='../organized_project',
                       help='Input directory containing Python models')
    parser.add_argument('--output_dir',
                       default='assets/models',
                       help='Output directory for TFLite models')

    args = parser.parse_args()

    # ëª¨ë¸ ë³€í™˜ ì‹¤í–‰
    converter = ModelConverter(args.input_dir, args.output_dir)
    model_paths = converter.convert_all()

    print("\n" + "="*50)
    print("ğŸ“± Mobile Model Conversion Complete!")
    print("="*50)
    print(f"ğŸ“ Output directory: {args.output_dir}")
    print("ğŸ“‹ Generated files:")
    for name, path in model_paths.items():
        print(f"  â€¢ {name}: {Path(path).name}")
    print(f"  â€¢ model_metadata.json")
    print("\nğŸ’¡ Next steps:")
    print("  1. Copy models to Flutter app assets/models/")
    print("  2. Update pubspec.yaml to include model assets")
    print("  3. Test models in Flutter app")

if __name__ == "__main__":
    main()