# ê²€ì¶œ ì•Œê³ ë¦¬ì¦˜ ê°œì„  ì•„ì´ë””ì–´

**í˜„ì¬ ì„±ëŠ¥**: 76.6% (Baseline Z-score, 3 features)
**ëª©í‘œ**: 80-85% ë‹¬ì„±

---

## í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### ê°•ì 
- âœ… ë‹¨ìˆœí•˜ê³  í•´ì„ ê°€ëŠ¥ (Z-score threshold)
- âœ… í•™ìŠµ ë¶ˆí•„ìš” (baselineë§Œ ê³„ì‚°)
- âœ… 3ê°œ featuresë¡œ íš¨ìœ¨ì 

### ì•½ì 
- âŒ ì„ í˜• ë¶„ë¦¬ë§Œ ê°€ëŠ¥ (Z > threshold)
- âŒ Feature ê°„ ìƒí˜¸ì‘ìš© ë¬´ì‹œ
- âŒ ê°œë³„ ë³‘ë¦¬ íŠ¹ì„± ê³ ë ¤ ì•ˆ í•¨
- âŒ Sensitivity ë‚®ìŒ (65.9%) - 34%ë¥¼ ë†“ì¹¨

### ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„
```
False Negatives (31ëª…): ë³‘ì  ë³´í–‰ì„ ì •ìƒìœ¼ë¡œ ì˜¤íŒ
  â†’ ë³´ìƒëœ ë³‘ì  ë³´í–‰ (compensated pathology)
  â†’ Mild severity cases

False Positives (15ëª…): ì •ìƒì„ ë³‘ì ìœ¼ë¡œ ì˜¤íŒ
  â†’ ë§¤ìš° ë¹ ë¥´ê±°ë‚˜ ëŠë¦° ì •ìƒ ë³´í–‰
  â†’ ìš´ë™ ì„ ìˆ˜, ë…¸ì¸
```

---

## ì¹´í…Œê³ ë¦¬ 1: ì•Œê³ ë¦¬ì¦˜ ê°œì„  (í˜„ì¬ features ìœ ì§€)

### 1.1 Adaptive Thresholding (ì ì‘í˜• ì„ê³„ê°’)

**ì•„ì´ë””ì–´**: ë‚˜ì´, ì„±ë³„, BMIì— ë”°ë¼ ë‹¤ë¥¸ baseline ì‚¬ìš©

**í˜„ì¬**:
```python
# Single baseline for all
if Z > 1.5: pathological
```

**ê°œì„ **:
```python
# Age-stratified baselines
if age < 30:
    baseline = baseline_young
    threshold = 1.5
elif age < 60:
    baseline = baseline_middle
    threshold = 1.7  # More lenient
else:
    baseline = baseline_elderly
    threshold = 2.0  # Much more lenient
```

**ì˜ˆìƒ ê°œì„ **: +2-3% (íŠ¹íˆ elderlyì—ì„œ false positive ê°ì†Œ)

**ì¥ì **: ì—¬ì „íˆ í•´ì„ ê°€ëŠ¥, ë‹¨ìˆœ
**ë‹¨ì **: Age/gender metadata í•„ìš”

---

### 1.2 Feature-Weighted Z-score (ê°€ì¤‘ Z-score)

**ì•„ì´ë””ì–´**: Cohen's dì— ë¹„ë¡€í•˜ì—¬ featuresì— ê°€ì¤‘ì¹˜ ë¶€ì—¬

**í˜„ì¬**:
```python
# Equal weighting
Z = (z_cadence + z_variability + z_irregularity) / 3
```

**ê°œì„ **:
```python
# Weight by discriminative power (Cohen's d)
w_cadence = 0.85       # d = 0.85 (LARGE)
w_variability = 0.35   # d = 0.35 (SMALL)
w_irregularity = 0.51  # d = 0.51 (MEDIUM)

# Normalize weights
total = w_cadence + w_variability + w_irregularity  # = 1.71
w_cadence_norm = 0.85/1.71 = 0.50
w_variability_norm = 0.35/1.71 = 0.20
w_irregularity_norm = 0.51/1.71 = 0.30

# Weighted Z-score
Z_weighted = 0.50*z_cadence + 0.20*z_variability + 0.30*z_irregularity
```

**ì˜ˆìƒ ê°œì„ **: +3-5% (cadence signal ë” ê°•ì¡°)

**ì¥ì **: ì—¬ì „íˆ í•´ì„ ê°€ëŠ¥, ì‰½ê²Œ êµ¬í˜„
**ë‹¨ì **: Overfitting ìœ„í—˜ (ì‘ì€ ë°ì´í„°ì…‹)

---

### 1.3 Mahalanobis Distance (ë‹¤ë³€ëŸ‰ ê±°ë¦¬)

**ì•„ì´ë””ì–´**: Feature ê°„ covariance ê³ ë ¤í•œ ê±°ë¦¬ ê³„ì‚°

**í˜„ì¬**: ê° feature ë…ë¦½ì ìœ¼ë¡œ Z-score ê³„ì‚°
**ê°œì„ **: Feature ê°„ ìƒê´€ê´€ê³„ ê³ ë ¤

```python
import numpy as np
from scipy.spatial.distance import mahalanobis

# Build baseline covariance matrix
normal_features = np.array([[cadence, var, irreg] for pattern in normal_patterns])
mean = np.mean(normal_features, axis=0)
cov = np.cov(normal_features.T)

# Compute Mahalanobis distance for test pattern
test_features = [test_cadence, test_var, test_irreg]
dist = mahalanobis(test_features, mean, np.linalg.inv(cov))

# Classify
if dist > threshold:
    prediction = 'pathological'
```

**ì˜ˆìƒ ê°œì„ **: +2-4% (feature correlation ê³ ë ¤)

**ì¥ì **: í†µê³„ì ìœ¼ë¡œ ë” rigorous, ì—¬ì „íˆ í•´ì„ ê°€ëŠ¥
**ë‹¨ì **: Covariance matrix ì¶”ì • (ì‘ì€ nì—ì„œ ë¶ˆì•ˆì •)

---

### 1.4 Confidence-Based Classification (ì‹ ë¢°ë„ ê¸°ë°˜)

**ì•„ì´ë””ì–´**: Borderline casesë¥¼ "uncertain"ìœ¼ë¡œ ë¶„ë¥˜

**í˜„ì¬**: Binary classification (normal or pathological)

**ê°œì„ **: 3-class classification
```python
if Z < 1.2:
    prediction = 'normal' (high confidence)
elif 1.2 <= Z < 1.8:
    prediction = 'uncertain' (refer for assessment)
else:
    prediction = 'pathological' (high confidence)
```

**ì˜ˆìƒ ê°œì„ **: AccuracyëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ clinical utility ì¦ê°€
- High confidence normal: 95% specificity
- High confidence pathological: 85% sensitivity
- Uncertain: Human review

**ì¥ì **: ì„ìƒì ìœ¼ë¡œ ë§¤ìš° ìœ ìš©
**ë‹¨ì **: 3-class evaluation metrics í•„ìš”

---

## ì¹´í…Œê³ ë¦¬ 2: Feature Engineering (ìƒˆë¡œìš´ features)

### 2.1 Full-Body Kinematics Features

**ì•„ì´ë””ì–´**: Heel ì™¸ì— ë‹¤ë¥¸ body parts ì¶”ê°€

**ìƒˆë¡œìš´ features (Cohen's d ì˜ˆìƒ)**:

**1. Stride Length (ë³´í­)**:
```python
# Hip-to-ankle distance at heel strike
stride_length = distance(hip_position, heel_position_at_peak)
```
- Normal: 0.8-1.2m
- Pathological: 0.4-0.8m (shorter)
- **Expected Cohen's d: 0.9-1.1** (LARGE)

**2. Trunk Sway (ëª¸í†µ í”ë“¤ë¦¼)**:
```python
# Shoulder lateral movement
shoulder_left = landmark[11]
shoulder_right = landmark[12]
shoulder_center_x = (shoulder_left.x + shoulder_right.x) / 2

trunk_sway = np.std(shoulder_center_x_trajectory)
```
- Normal: Low sway (< 0.05)
- Pathological: High sway (> 0.10)
- **Expected Cohen's d: 0.7-0.9** (MEDIUM-LARGE)

**3. Arm Swing Asymmetry (íŒ” í”ë“¤ë¦¼ ë¹„ëŒ€ì¹­)**:
```python
# Wrist vertical movement amplitude
left_wrist_amplitude = max(wrist_left_y) - min(wrist_left_y)
right_wrist_amplitude = max(wrist_right_y) - min(wrist_right_y)

arm_asymmetry = abs(left_wrist_amplitude - right_wrist_amplitude)
```
- Normal: Low asymmetry (< 0.1)
- Pathological: High asymmetry (> 0.2, e.g., Parkinson's, stroke)
- **Expected Cohen's d: 0.6-0.8** (MEDIUM)

**4. Step Width Variability (ë³´í­ í­ ë³€ë™ì„±)**:
```python
# Lateral distance between heel strikes
step_widths = [distance_x(left_heel_peak, right_heel_peak) for peaks]
step_width_var = np.std(step_widths) / np.mean(step_widths)
```
- Normal: Low variability (< 0.15)
- Pathological: High variability (> 0.25, fall risk)
- **Expected Cohen's d: 0.7-0.9** (MEDIUM-LARGE)

**ì˜ˆìƒ ê°œì„ **: +4-8% (3 features â†’ 6-7 features with d>0.7)

**ì£¼ì˜ì‚¬í•­**:
- Feature selection í•„ìˆ˜ (Cohen's d > 0.7ë§Œ ì‚¬ìš©)
- Correlation check (|r| < 0.7)
- Stride lengthì™€ cadence ìƒê´€ê´€ê³„ í™•ì¸ í•„ìš”

---

### 2.2 Temporal Pattern Features (ì‹œê³„ì—´ íŒ¨í„´)

**ì•„ì´ë””ì–´**: ì‹œê°„ì— ë”°ë¥¸ pattern shape ë¶„ì„

**1. Gait Cycle Symmetry (ë³´í–‰ ì£¼ê¸° ëŒ€ì¹­ì„±)**:
```python
# Correlation between left and right heel trajectories
from scipy.stats import pearsonr

# Time-align trajectories
r, p = pearsonr(heel_left_aligned, heel_right_aligned)

symmetry = r  # Range: -1 to 1
```
- Normal: High symmetry (r > 0.85)
- Pathological: Low symmetry (r < 0.70, e.g., hemiplegia)
- **Expected Cohen's d: 0.8-1.0** (LARGE for asymmetric pathologies)

**2. Peak Sharpness (í”¼í¬ ë‚ ì¹´ë¡œì›€)**:
```python
# Kurtosis of heel height trajectory
from scipy.stats import kurtosis

peak_sharpness = kurtosis(heel_height)
```
- Normal: Sharp peaks (kurtosis > 0)
- Pathological: Flat peaks (kurtosis < 0, shuffling gait)
- **Expected Cohen's d: 0.5-0.7** (MEDIUM)

**3. Harmonic Ratio (ì¡°í™”ë¹„)**:
```python
# FFT of heel height trajectory
from scipy.fft import fft

fft_vals = fft(heel_height)
power = np.abs(fft_vals)**2

# Ratio of first harmonic to higher harmonics
harmonic_ratio = power[1] / np.sum(power[2:6])
```
- Normal: High ratio (smooth gait)
- Pathological: Low ratio (irregular gait)
- **Expected Cohen's d: 0.6-0.8** (MEDIUM)

**ì˜ˆìƒ ê°œì„ **: +2-4% (temporal patterns ì¶”ê°€)

---

### 2.3 Multi-View Fusion (ë‹¤ì¤‘ ì‹œì )

**ì•„ì´ë””ì–´**: ì •ë©´(frontal) + ì¸¡ë©´(sagittal) ë™ì‹œ ë¶„ì„

**Frontal viewì—ì„œë§Œ ë³´ì´ëŠ” features**:
```python
# 1. Lateral trunk lean
trunk_lean = angle(shoulder_center, hip_center, vertical)

# 2. Step width
step_width = distance_x(left_heel, right_heel)

# 3. Knee valgus/varus
knee_angle_frontal = angle(hip, knee, ankle) - 180
```

**Fusion ë°©ë²•**:
```python
# Late fusion (combine scores)
Z_sagittal = compute_z_score(sagittal_features)
Z_frontal = compute_z_score(frontal_features)

Z_combined = (Z_sagittal + Z_frontal) / 2

if Z_combined > threshold:
    prediction = 'pathological'
```

**ì˜ˆìƒ ê°œì„ **: +3-6% (complementary information)

**ë‹¨ì **: 2ê°œ ì˜ìƒ í•„ìš” (deployment ë³µì¡ë„ ì¦ê°€)

---

## ì¹´í…Œê³ ë¦¬ 3: Machine Learning Approaches

### 3.1 Logistic Regression (í•´ì„ ê°€ëŠ¥í•œ ML)

**ì•„ì´ë””ì–´**: Feature ê°„ ë¹„ì„ í˜• ì¡°í•© í•™ìŠµ

```python
from sklearn.linear_model import LogisticRegression

# Features
X = [[cadence, variability, irregularity] for pattern in patterns]
y = [0 if normal else 1 for pattern in patterns]

# Train
model = LogisticRegression(penalty='l2', C=1.0)
model.fit(X, y)

# Coefficients (interpretable!)
print(f"Cadence weight: {model.coef_[0][0]}")
print(f"Variability weight: {model.coef_[0][1]}")
print(f"Irregularity weight: {model.coef_[0][2]}")

# Predict
prob = model.predict_proba(test_features)[0][1]
if prob > 0.5:
    prediction = 'pathological'
```

**ì˜ˆìƒ ê°œì„ **: +3-5% (non-linear decision boundary)

**ì¥ì **:
- ì—¬ì „íˆ í•´ì„ ê°€ëŠ¥ (coefficients)
- Feature importance ìë™ í•™ìŠµ
- Probability output (confidence)

**ë‹¨ì **:
- Training í•„ìš”
- Overfitting ìœ„í—˜ (ì‘ì€ n=187)
- Cross-validation í•„ìˆ˜

---

### 3.2 Random Forest (Feature Importance)

**ì•„ì´ë””ì–´**: Decision tree ensembleë¡œ feature interaction í•™ìŠµ

```python
from sklearn.ensemble import RandomForestClassifier

# Train
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,  # Prevent overfitting
    min_samples_leaf=10,
    random_state=42
)
model.fit(X, y)

# Feature importance
importances = model.feature_importances_
print(f"Cadence: {importances[0]:.3f}")
print(f"Variability: {importances[1]:.3f}")
print(f"Irregularity: {importances[2]:.3f}")

# Predict
prob = model.predict_proba(test_features)[0][1]
```

**ì˜ˆìƒ ê°œì„ **: +5-8% (non-linear, feature interactions)

**ì¥ì **:
- Feature interactions ìë™ í•™ìŠµ
- Feature importance ì¶œë ¥
- Robust to outliers

**ë‹¨ì **:
- "Black box" (í•´ì„ ì–´ë ¤ì›€)
- Overfitting ìœ„í—˜ (small n)
- Deployment ë³µì¡ (100 trees)

---

### 3.3 Support Vector Machine (Non-linear Boundary)

**ì•„ì´ë””ì–´**: Kernel trickìœ¼ë¡œ non-linear decision boundary

```python
from sklearn.svm import SVC

# Train with RBF kernel
model = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    probability=True
)
model.fit(X, y)

# Predict
prob = model.predict_proba(test_features)[0][1]
```

**ì˜ˆìƒ ê°œì„ **: +4-7% (complex boundaries)

**ì¥ì **: Non-linear separation
**ë‹¨ì **:
- Hyperparameter tuning í•„ìš”
- Not interpretable
- Small n ìœ„í—˜

---

### 3.4 Gradient Boosting (XGBoost/LightGBM)

**ì•„ì´ë””ì–´**: ìµœê³  ì„±ëŠ¥ì˜ classical ML

```python
from xgboost import XGBClassifier

# Train
model = XGBClassifier(
    n_estimators=50,
    max_depth=3,  # Shallow to prevent overfitting
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
model.fit(X, y)

# Feature importance
importances = model.feature_importances_

# Predict
prob = model.predict_proba(test_features)[0][1]
```

**ì˜ˆìƒ ê°œì„ **: +6-10% (best classical ML)

**ì¥ì **:
- State-of-the-art performance
- Feature importance
- Handles complex interactions

**ë‹¨ì **:
- Overfitting ìœ„í—˜ (ì‘ì€ n)
- Hyperparameter tuning í•„ìˆ˜
- Less interpretable
- Deployment ë³µì¡

---

### 3.5 Deep Learning (LSTM for Time Series)

**ì•„ì´ë””ì–´**: ì‹œê³„ì—´ ì „ì²´ë¥¼ inputìœ¼ë¡œ end-to-end í•™ìŠµ

```python
import tensorflow as tf
from tensorflow.keras import layers

# Model
model = tf.keras.Sequential([
    layers.LSTM(64, input_shape=(n_frames, 2)),  # 2 = left/right heel
    layers.Dropout(0.5),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Input: raw heel height trajectories (no manual features!)
X = [pattern['heel_height_left'] + pattern['heel_height_right'] for pattern in patterns]
y = [0 if normal else 1 for pattern in patterns]

# Train
model.fit(X, y, epochs=50, batch_size=16, validation_split=0.2)

# Predict
prob = model.predict(test_trajectory)[0][0]
```

**ì˜ˆìƒ ê°œì„ **: +8-15% (learns optimal features)

**ì¥ì **:
- End-to-end learning
- Learns optimal features automatically
- Captures temporal dependencies

**ë‹¨ì **:
- **ì‘ì€ ë°ì´í„°ì…‹ (n=187)ì—ì„œ overfitting ì‹¬ê°**
- Black box (ì™„ì „íˆ í•´ì„ ë¶ˆê°€)
- Deployment ë³µì¡ (model weights)
- Training ëŠë¦¼

**ê¶Œì¥**: ë°ì´í„° 500+ í™•ë³´ í›„ ì‹œë„

---

## ì¹´í…Œê³ ë¦¬ 4: Ensemble Methods (ì•™ìƒë¸”)

### 4.1 Stacking (ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©)

**ì•„ì´ë””ì–´**: ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì˜ ì˜ˆì¸¡ì„ meta-learnerë¡œ ì¡°í•©

```python
from sklearn.ensemble import StackingClassifier

# Base models
estimators = [
    ('z_score', ZScoreClassifier()),  # Custom
    ('logistic', LogisticRegression()),
    ('rf', RandomForestClassifier(n_estimators=50, max_depth=5)),
]

# Meta-learner
stacking_model = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),
    cv=5
)

stacking_model.fit(X, y)
```

**ì˜ˆìƒ ê°œì„ **: +5-8% (ê° ëª¨ë¸ì˜ ê°•ì  ê²°í•©)

**ì¥ì **: Best of all models
**ë‹¨ì **: ë³µì¡, í•´ì„ ì–´ë ¤ì›€

---

### 4.2 Voting Classifier (ë‹¤ìˆ˜ê²°)

**ì•„ì´ë””ì–´**: ì—¬ëŸ¬ ëª¨ë¸ì˜ voteë¡œ ê²°ì •

```python
from sklearn.ensemble import VotingClassifier

# Models
model1 = ZScoreClassifier()  # 76.6%
model2 = LogisticRegression()  # ~80%
model3 = RandomForestClassifier()  # ~82%

# Voting
voting_model = VotingClassifier(
    estimators=[('z', model1), ('lr', model2), ('rf', model3)],
    voting='soft',  # Probability averaging
    weights=[1, 2, 2]  # Trust ML more than baseline
)

voting_model.fit(X, y)
```

**ì˜ˆìƒ ê°œì„ **: +4-7% (robust to individual model errors)

---

## ì¹´í…Œê³ ë¦¬ 5: Pathology-Specific Detection

### 5.1 Hierarchical Classification (ê³„ì¸µì  ë¶„ë¥˜)

**ì•„ì´ë””ì–´**: Stage 1 (normal vs pathological) â†’ Stage 2 (specific pathology)

```python
# Stage 1: Binary classification
if Z < 1.5:
    return 'normal'

# Stage 2: Multi-class classification (pathological type)
pathology_features = extract_pathology_specific_features(pattern)

if asymmetry > 0.3 and cadence < 150:
    return 'hemiplegia (stroke)'
elif trunk_sway > 0.15 and step_width_var > 0.3:
    return 'ataxia (cerebellar)'
elif cadence < 100 and stride_length < 0.5:
    return 'parkinsonian gait'
else:
    return 'unspecified pathological'
```

**ì˜ˆìƒ ê°œì„ **: Stage 1ì€ ë¹„ìŠ·, but clinical utility ì¦ê°€ (specific diagnosis)

---

### 5.2 One-Class SVM (Anomaly Detection)

**ì•„ì´ë””ì–´**: Normalë§Œ í•™ìŠµ, pathologicalì„ outlierë¡œ ê²€ì¶œ

```python
from sklearn.svm import OneClassSVM

# Train on normal only!
normal_features = [extract_features(p) for p in normal_patterns]

model = OneClassSVM(kernel='rbf', gamma='auto', nu=0.1)
model.fit(normal_features)

# Predict
score = model.decision_function(test_features)
if score < 0:
    prediction = 'pathological' (outlier)
else:
    prediction = 'normal'
```

**ì˜ˆìƒ ê°œì„ **: +2-4% (íŠ¹íˆ rare pathologies)

**ì¥ì **: Normalë§Œ í•™ìŠµ (unbalanced dataì— ê°•í•¨)
**ë‹¨ì **: Hyperparameter tuning ì–´ë ¤ì›€

---

## ì¶”ì²œ ë‹¨ê³„ë³„ ë¡œë“œë§µ

### Phase 1: Quick Wins (1-2ì£¼, +5-8% ì˜ˆìƒ)

**ìš°ì„ ìˆœìœ„ 1**: Feature-Weighted Z-score (ê°€ì¤‘ Z-score)
- êµ¬í˜„: 1ì¼
- Expected: +3-5%
- Reason: ë‹¨ìˆœ, í•´ì„ ê°€ëŠ¥, Cohen's d í™œìš©

**ìš°ì„ ìˆœìœ„ 2**: Confidence-Based Classification (3-class)
- êµ¬í˜„: 2ì¼
- Expected: Clinical utility ì¦ê°€
- Reason: Borderline cases ì²˜ë¦¬

**ìš°ì„ ìˆœìœ„ 3**: Age-Stratified Baseline (ë‚˜ì´ë³„ baseline)
- êµ¬í˜„: 3ì¼ (ë‚˜ì´ metadata ìˆ˜ì§‘ í•„ìš”)
- Expected: +2-3%
- Reason: False positive ê°ì†Œ (elderly)

**ì˜ˆìƒ ê²°ê³¼**: 76.6% â†’ 81-84%

---

### Phase 2: Feature Engineering (2-4ì£¼, +4-8% ì¶”ê°€)

**ìš°ì„ ìˆœìœ„ 1**: Stride Length (ë³´í­)
- êµ¬í˜„: 5ì¼
- Expected Cohen's d: 0.9-1.1 (LARGE)
- Expected: +3-5%

**ìš°ì„ ìˆœìœ„ 2**: Trunk Sway (ëª¸í†µ í”ë“¤ë¦¼)
- êµ¬í˜„: 3ì¼
- Expected Cohen's d: 0.7-0.9
- Expected: +2-3%

**ìš°ì„ ìˆœìœ„ 3**: Gait Cycle Symmetry (ëŒ€ì¹­ì„±)
- êµ¬í˜„: 2ì¼
- Expected Cohen's d: 0.8-1.0
- Expected: +2-3%

**Feature selection**:
- ê° featureì˜ Cohen's d ê³„ì‚°
- d > 0.7ë§Œ ì¶”ê°€
- Correlation < 0.7 í™•ì¸

**ì˜ˆìƒ ê²°ê³¼**: 81-84% â†’ 85-89%

---

### Phase 3: Machine Learning (4-6ì£¼, +3-5% ì¶”ê°€)

**ìš°ì„ ìˆœìœ„ 1**: Logistic Regression
- êµ¬í˜„: 1ì£¼ (cross-validation í¬í•¨)
- Expected: +3-5%
- Reason: í•´ì„ ê°€ëŠ¥, ì ì€ overfitting

**ìš°ì„ ìˆœìœ„ 2**: Random Forest (if data > 300)
- êµ¬í˜„: 1ì£¼
- Expected: +4-6%
- Reason: Feature interactions

**Cross-validation**:
- Stratified 5-fold CV
- Test on unseen 20%
- Report mean Â± std

**ì˜ˆìƒ ê²°ê³¼**: 85-89% â†’ 88-92%

---

### Phase 4: Advanced (ì¥ê¸°, data í™•ë³´ í›„)

**ë°ì´í„° í™•ë³´ ëª©í‘œ**: 500+ patterns
- í˜„ì¬: 187 patterns
- ëª©í‘œ: 500+ (clinical trial í†µí•´ ìˆ˜ì§‘)

**ì´í›„ ì‹œë„**:
- XGBoost/LightGBM
- LSTM/Transformer
- Multi-view fusion
- Ensemble methods

**ì˜ˆìƒ ê²°ê³¼**: 88-92% â†’ 90-95%

---

## ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥í•œ Top 3 ì¶”ì²œ

### ğŸ¥‡ #1: Feature-Weighted Z-score

**Why**:
- êµ¬í˜„ ë§¤ìš° ê°„ë‹¨ (30ë¶„)
- í•´ì„ ê°€ëŠ¥ ìœ ì§€
- Cohen's d ì§ì ‘ í™œìš©
- Overfitting ìœ„í—˜ ë‚®ìŒ

**Code**:
```python
def weighted_z_score(features, baseline):
    # Weights from Cohen's d
    w_cadence = 0.50
    w_variability = 0.20
    w_irregularity = 0.30

    z_cadence = abs(features.cadence - baseline['cadence_mean']) / baseline['cadence_std']
    z_var = abs(features.variability - baseline['variability_mean']) / baseline['variability_std']
    z_irreg = abs(features.irregularity - baseline['irregularity_mean']) / baseline['irregularity_std']

    Z = w_cadence*z_cadence + w_variability*z_var + w_irregularity*z_irreg
    return Z
```

**Expected**: 76.6% â†’ 79-81%

---

### ğŸ¥ˆ #2: Stride Length Feature ì¶”ê°€

**Why**:
- Cohen's d ì˜ˆìƒ: 0.9-1.1 (LARGE!)
- MediaPipeì—ì„œ ê³„ì‚° ê°€ëŠ¥ (hip, ankle landmarks)
- ì„ìƒì ìœ¼ë¡œ ì˜ë¯¸ ìˆìŒ
- ë‹¨ì¼ featureë¡œ í° ê°œì„  ê°€ëŠ¥

**Code**:
```python
def compute_stride_length(pattern):
    hip_y = pattern['hip_center_y']
    ankle_y = pattern['ankle_y']

    # Distance at heel strike (peak)
    peaks, _ = find_peaks(pattern['heel_height_left'])

    stride_lengths = []
    for peak in peaks:
        hip_pos = [pattern['hip_x'][peak], hip_y[peak]]
        ankle_pos = [pattern['ankle_x'][peak], ankle_y[peak]]
        stride_lengths.append(np.linalg.norm(np.array(hip_pos) - np.array(ankle_pos)))

    return np.mean(stride_lengths)
```

**Expected**: 79-81% â†’ 82-85%

---

### ğŸ¥‰ #3: Logistic Regression (4 features)

**Why**:
- Feature ê°„ interaction í•™ìŠµ
- í•´ì„ ê°€ëŠ¥ (coefficients)
- Probability output
- ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ ì•ˆì •ì 

**Code**:
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# Features: cadence, variability, irregularity, stride_length
X = [[p.cadence, p.variability, p.irregularity, p.stride_length] for p in patterns]
y = [0 if p.label=='normal' else 1 for p in patterns]

# Train with L2 regularization
model = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)

# Cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"CV Accuracy: {scores.mean():.3f} Â± {scores.std():.3f}")

# Train on all data
model.fit(X, y)

# Coefficients (interpretable!)
print("Feature importances:")
for i, name in enumerate(['cadence', 'variability', 'irregularity', 'stride_length']):
    print(f"{name}: {model.coef_[0][i]:.3f}")
```

**Expected**: 82-85% â†’ 85-88%

---

## êµ¬í˜„ ìˆœì„œ ìš”ì•½

```
Week 1: Feature-Weighted Z-score
         â†“
       79-81%

Week 2-3: Stride Length ì¶”ê°€
         â†“
       82-85%

Week 4-5: Logistic Regression
         â†“
       85-88%

Week 6+: Trunk Sway, Symmetry ì¶”ê°€
         â†“
       88-92% (if data sufficient)
```

---

## ìµœì¢… ê¶Œì¥ì‚¬í•­

**ì¦‰ì‹œ ì‹œì‘** (ì´ë²ˆ ì£¼):
1. âœ… Feature-Weighted Z-score (30ë¶„ êµ¬í˜„, +3-5%)
2. âœ… Stride Length ì¶”ê°€ (2ì¼ êµ¬í˜„, +3-5%)

**ë‹¤ìŒ ë‹¨ê³„** (2-4ì£¼):
3. âœ… Logistic Regression (1ì£¼ êµ¬í˜„, +3-5%)
4. âœ… Trunk Sway or Symmetry (1-2ì£¼ êµ¬í˜„, +2-4%)

**ëª©í‘œ ë‹¬ì„± ì˜ˆìƒ**:
- í˜„ì¬: 76.6%
- ë‹¨ê¸° (4ì£¼): 85-88%
- ì¤‘ê¸° (8ì£¼): 88-92% (ë°ì´í„° ì¶©ë¶„ ì‹œ)

**í•µì‹¬ ì›ì¹™ ìœ ì§€**:
- âœ… Cohen's d > 0.7 featuresë§Œ ì¶”ê°€
- âœ… Correlation < 0.7 í™•ì¸
- âœ… í•´ì„ ê°€ëŠ¥ì„± ìœ ì§€ (clinicians trust)
- âœ… Cross-validation í•„ìˆ˜ (overfitting ë°©ì§€)

---

**íŒŒì¼**: ALGORITHM_IMPROVEMENT_IDEAS.md
**ì‘ì„±ì¼**: 2025-10-30
**ëª©í‘œ**: 76.6% â†’ 85-92% ë‹¬ì„±
**ì¶”ì²œ ì‹œì‘**: Feature-Weighted Z-score + Stride Length
